[
  {
    "objectID": "instruction_prompt.html",
    "href": "instruction_prompt.html",
    "title": "AI Success Factors: Engineering Trust in Deployments",
    "section": "",
    "text": "ChatGPT, please assist in drafting a blog post for AI Success Factors: Engineering Trust in Deployments. As a generative AI, your input serves as a starting point for discussion and is meticulously reviewed and edited by a team of AI researchers at the Laboratory for Assured AI Applications Development at the University of Notre Dame’s Center for Research Computing.\nThe intended audience includes students, professionals, and enthusiasts interested in the progression of AI, with a focus on the successful deployment of AI technologies. Each post aims to disseminate recent advancements and knowledge in AI, particularly in the fields of AI engineering, trust in AI, and knowledge engineering.\nFor this post, please generate content on the importance of trust in successful AI deployments. Begin with a general introduction to AI deployments, transition into the crucial role of trust, and provide real-world examples where the absence of trust led to issues. Discuss strategies for building trust in AI systems and conclude with insights on future trends in trust within the AI landscape. The tone should be academic yet accessible to our diverse readership."
  },
  {
    "objectID": "slides/tai_testing/index.html#real-world-machine-learning-pipelines",
    "href": "slides/tai_testing/index.html#real-world-machine-learning-pipelines",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Real World Machine Learning “Pipelines”",
    "text": "Real World Machine Learning “Pipelines”\n\n\nTwitter: Andrej Karpathy"
  },
  {
    "objectID": "slides/tai_testing/index.html#real-world-machine-learning-data-engine",
    "href": "slides/tai_testing/index.html#real-world-machine-learning-data-engine",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Real World Machine Learning “Data Engine”",
    "text": "Real World Machine Learning “Data Engine”\n\n\nTwitter: Andrej Karpathy"
  },
  {
    "objectID": "slides/tai_testing/index.html#what-is-testing-for-ai-vs-traditional-software-testing",
    "href": "slides/tai_testing/index.html#what-is-testing-for-ai-vs-traditional-software-testing",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "What is “Testing” for AI vs Traditional Software Testing",
    "text": "What is “Testing” for AI vs Traditional Software Testing\n\n\nLearn More: Effective testing for machine learning systems\nYoutube Discussion: MLOps Chat: How Should We Test ML Models? with Data Scientist Jeremy Jordan"
  },
  {
    "objectID": "slides/tai_testing/index.html#testing-software-1.0-vs-software-2.0",
    "href": "slides/tai_testing/index.html#testing-software-1.0-vs-software-2.0",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Testing “Software 1.0” vs “Software 2.0”",
    "text": "Testing “Software 1.0” vs “Software 2.0”\n\n\nLearn More: Effective testing for machine learning systems"
  },
  {
    "objectID": "slides/tai_testing/index.html#beyond-accuracy-behavioral-testing-of-nlp-models-with-checklist",
    "href": "slides/tai_testing/index.html#beyond-accuracy-behavioral-testing-of-nlp-models-with-checklist",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "“Beyond Accuracy: Behavioral Testing of NLP Models with CheckList”",
    "text": "“Beyond Accuracy: Behavioral Testing of NLP Models with CheckList”\n\n\nLearn More: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\nGitHub:Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"
  },
  {
    "objectID": "slides/tai_testing/index.html#examples-of-real-world-testing",
    "href": "slides/tai_testing/index.html#examples-of-real-world-testing",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Examples of “real world” testing",
    "text": "Examples of “real world” testing\n\n\nLearn More: Microsoft Recommenders GitHub"
  },
  {
    "objectID": "slides/tai_testing/index.html#examples-of-real-world-testing-as-a-starting-point",
    "href": "slides/tai_testing/index.html#examples-of-real-world-testing-as-a-starting-point",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Examples of “real world” testing as a starting point",
    "text": "Examples of “real world” testing as a starting point\n\n\nLearn More: Microsoft Recommenders GitHub"
  },
  {
    "objectID": "slides/tai_testing/index.html#chatgpt-behavior-drift",
    "href": "slides/tai_testing/index.html#chatgpt-behavior-drift",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "ChatGPT Behavior Drift?",
    "text": "ChatGPT Behavior Drift?\n\n\nLearn More: How is ChatGPT’s behavior changing over time?"
  },
  {
    "objectID": "slides/tai_testing/index.html#chatgpt-behavior-drift-1",
    "href": "slides/tai_testing/index.html#chatgpt-behavior-drift-1",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "ChatGPT Behavior Drift?",
    "text": "ChatGPT Behavior Drift?\n\n\nLearn More: How is ChatGPT’s behavior changing over time?"
  },
  {
    "objectID": "slides/tai_testing/index.html#tracking-llm-agent-abilities",
    "href": "slides/tai_testing/index.html#tracking-llm-agent-abilities",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Tracking LLM “Agent Abilities”",
    "text": "Tracking LLM “Agent Abilities”\n\n\nLearn More: AgentBench: Evaluating LLMs as Agents"
  },
  {
    "objectID": "slides/tai_testing/index.html#tracking-llm-agent-abilities-1",
    "href": "slides/tai_testing/index.html#tracking-llm-agent-abilities-1",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Tracking LLM “Agent Abilities”",
    "text": "Tracking LLM “Agent Abilities”\n\n\nLearn More: AgentBench: Evaluating LLMs as Agents"
  },
  {
    "objectID": "slides/tai_testing/index.html#testing-in-the-context-of-cicd",
    "href": "slides/tai_testing/index.html#testing-in-the-context-of-cicd",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Testing in the context of CI/CD",
    "text": "Testing in the context of CI/CD\n\n\nLearn More: Testing stages in continuous integration and continuous delivery"
  },
  {
    "objectID": "slides/tai_testing/index.html#testing-in-the-context-of-sboms",
    "href": "slides/tai_testing/index.html#testing-in-the-context-of-sboms",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Testing in the context of SBoMs",
    "text": "Testing in the context of SBoMs\n\n\nLearn More: Testing stages in continuous integration and continuous delivery"
  },
  {
    "objectID": "slides/tai_testing/index.html#other-updates",
    "href": "slides/tai_testing/index.html#other-updates",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Other Updates",
    "text": "Other Updates"
  },
  {
    "objectID": "slides/tai_testing/index.html#deploying-llama2-to-aws",
    "href": "slides/tai_testing/index.html#deploying-llama2-to-aws",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Deploying llama2 to AWS",
    "text": "Deploying llama2 to AWS\n\n\nLearn More: Deploy Llama 2 7B/13B/70B on Amazon SageMaker"
  },
  {
    "objectID": "slides/tai_testing/index.html#deploying-llama2-to-aws-1",
    "href": "slides/tai_testing/index.html#deploying-llama2-to-aws-1",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Deploying llama2 to AWS",
    "text": "Deploying llama2 to AWS\n\n\nLearn More: Deploy Llama 2 7B/13B/70B on Amazon SageMaker"
  },
  {
    "objectID": "slides/tai_testing/index.html#juypter-ai",
    "href": "slides/tai_testing/index.html#juypter-ai",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Juypter AI",
    "text": "Juypter AI\n\n\nLearn More: Generative AI in Jupyter\nGitHub: A generative AI extension for JupyterLab"
  },
  {
    "objectID": "slides/tai_testing/index.html#juypter-ai-1",
    "href": "slides/tai_testing/index.html#juypter-ai-1",
    "title": "Things that concern Dr. Vardeman: Testing",
    "section": "Juypter AI",
    "text": "Juypter AI\n\n\nLearn More: Generative AI in Jupyter"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#kg-construction-pipeline",
    "href": "slides/kg-llamaindex/index.html#kg-construction-pipeline",
    "title": "KG Construction",
    "section": "KG Construction Pipeline",
    "text": "KG Construction Pipeline"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap",
    "href": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap",
    "title": "KG Construction",
    "section": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "text": "Unifying Large Language Models and Knowledge Graphs: A Roadmap\n\n\nLearn more: ArXiv Paper"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap-1",
    "href": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap-1",
    "title": "KG Construction",
    "section": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "text": "Unifying Large Language Models and Knowledge Graphs: A Roadmap\n\n\nLearn more: ArXiv Paper"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap-2",
    "href": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap-2",
    "title": "KG Construction",
    "section": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "text": "Unifying Large Language Models and Knowledge Graphs: A Roadmap\n\n\nLearn more: ArXiv Paper"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap-3",
    "href": "slides/kg-llamaindex/index.html#unifying-large-language-models-and-knowledge-graphs-a-roadmap-3",
    "title": "KG Construction",
    "section": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "text": "Unifying Large Language Models and Knowledge Graphs: A Roadmap\n\n\nLearn more: ArXiv Paper"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#llm-kg-construction-frameworks-llamaindex",
    "href": "slides/kg-llamaindex/index.html#llm-kg-construction-frameworks-llamaindex",
    "title": "KG Construction",
    "section": "LLM KG Construction Frameworks Llamaindex",
    "text": "LLM KG Construction Frameworks Llamaindex\n\nLearn more: Llamaindex"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#rebel-large-model",
    "href": "slides/kg-llamaindex/index.html#rebel-large-model",
    "title": "KG Construction",
    "section": "Rebel Large Model",
    "text": "Rebel Large Model\n#| echo: true\nfrom transformers import pipeline\n\ntriplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n# We need to use the tokenizer manually since we need special tokens.\nextracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(\"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\", return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\nprint(extracted_text[0])\n# Function to parse the generated text and extract the triplets\ndef extract_triplets(text):\n    triplets = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    for token in text.replace(\"&lt;s&gt;\", \"\").replace(\"&lt;pad&gt;\", \"\").replace(\"&lt;/s&gt;\", \"\").split():\n        if token == \"&lt;triplet&gt;\":\n            current = 't'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n                relation = ''\n            subject = ''\n        elif token == \"&lt;subj&gt;\":\n            current = 's'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n            object_ = ''\n        elif token == \"&lt;obj&gt;\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n    return triplets\nextracted_triplets = extract_triplets(extracted_text[0])\nprint(extracted_triplets)\n\nLearn more: Rebel Large"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#llamaindex-with-rebel",
    "href": "slides/kg-llamaindex/index.html#llamaindex-with-rebel",
    "title": "KG Construction",
    "section": "LlamaIndex with Rebel",
    "text": "LlamaIndex with Rebel\n\n\nLearn more: Llamaindex with Rebel Colab Notebook"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#llamaindex-with-rebel-and-neo4j",
    "href": "slides/kg-llamaindex/index.html#llamaindex-with-rebel-and-neo4j",
    "title": "KG Construction",
    "section": "LlamaIndex with Rebel and Neo4j",
    "text": "LlamaIndex with Rebel and Neo4j\n\n\nLearn more: Neo4j Graph Store\nGitHub: Tomaz Bratanic"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#weaviate-with-llama-2-and-llamaindex",
    "href": "slides/kg-llamaindex/index.html#weaviate-with-llama-2-and-llamaindex",
    "title": "KG Construction",
    "section": "Weaviate With Llama 2 and Llamaindex",
    "text": "Weaviate With Llama 2 and Llamaindex\n\n\nLearn more: Welcome to the quick notebook on using Llama 2"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#weaviate-knowledge-graphs",
    "href": "slides/kg-llamaindex/index.html#weaviate-knowledge-graphs",
    "title": "KG Construction",
    "section": "Weaviate “Knowledge Graphs”",
    "text": "Weaviate “Knowledge Graphs”\n\n\nExample: NASA Graph-Enabled Vector Search"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#weaviate-knowledge-graphs-1",
    "href": "slides/kg-llamaindex/index.html#weaviate-knowledge-graphs-1",
    "title": "KG Construction",
    "section": "Weaviate “Knowledge Graphs”",
    "text": "Weaviate “Knowledge Graphs”\n\n\nExample: NASA Graph-Enabled Vector Search"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#aws-neptune",
    "href": "slides/kg-llamaindex/index.html#aws-neptune",
    "title": "KG Construction",
    "section": "AWS Neptune",
    "text": "AWS Neptune\n\n\nLearn More: Amazon Neptune"
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#amazon-science-refined",
    "href": "slides/kg-llamaindex/index.html#amazon-science-refined",
    "title": "KG Construction",
    "section": "Amazon Science ReFinED",
    "text": "Amazon Science ReFinED\n\n\nPaper: ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking\nPaper: Improving Entity Disambiguation by Reasoning over a Knowledge Base\nGitHub: ReFinED is an efficient and accurate entity linking (EL) system."
  },
  {
    "objectID": "slides/kg-llamaindex/index.html#amazon-science-kgqa",
    "href": "slides/kg-llamaindex/index.html#amazon-science-kgqa",
    "title": "KG Construction",
    "section": "Amazon Science KGQA",
    "text": "Amazon Science KGQA\n\n\nWebsite: Language models as controlled natural language semantic parsers for knowledge graph question answering GitHub: Language Models as Controlled Natural Language Semantic Parsers for Knowledge Graph Question Answering"
  },
  {
    "objectID": "slideindex.html",
    "href": "slideindex.html",
    "title": "Weekly Nugget Presentations",
    "section": "",
    "text": "Things that concern Dr. Vardeman: Testing\n\n\n\n\n\n\n\nframeworks\n\n\ntrustedAI\n\n\nTesting\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nCharles F Vardeman II\n\n\n\n\n\n\n  \n\n\n\n\nKG Construction\n\n\n\n\n\n\n\nKnowledge Engineering\n\n\nKG Construction\n\n\ntrustedAI\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nCharles F Vardeman II\n\n\n\n\n\n\n  \n\n\n\n\nTrust and Causal Reasoning\n\n\n\n\n\n\n\nframeworks\n\n\ntrustedAI\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nCharles F Vardeman II\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To AI Success Factors: Engineering Trust in Deployments",
    "section": "",
    "text": "This is the introduction to LA3D, Projects and Goals!\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have stumbled on this post, it is still in the rough draft state and under major revision.\n\n\nSome text will go here."
  },
  {
    "objectID": "posts/frameworks-reflection/index.html",
    "href": "posts/frameworks-reflection/index.html",
    "title": "Reflections on Trusted AI Frameworks after two years",
    "section": "",
    "text": "Note\n\n\n\nIf you have stumbled on this post, it is still in the rough draft state and under major revision."
  },
  {
    "objectID": "posts/frameworks-reflection/index.html#introduction-and-context-of-trusted-ai-tai",
    "href": "posts/frameworks-reflection/index.html#introduction-and-context-of-trusted-ai-tai",
    "title": "Reflections on Trusted AI Frameworks after two years",
    "section": "1 Introduction and Context of Trusted AI (TAI)",
    "text": "1 Introduction and Context of Trusted AI (TAI)\nIn an age where Artificial Intelligence (AI) is advancing at an unprecedented pace, trust in these intelligent systems is paramount—especially within the context of military operations. Welcome to a reflection on the Trusted AI (TAI) Frameworks Project, a collaborative initiative between Indiana University, University of Notre Dame, and CRANE Naval Surface Warfare Center as part of Scalable Asymmetric Lifecycle Engagement (SCALE) led by Purdue University. Our mission is to enhance the successful and trusted deployment of AI to the Navy and Marine Corps and fortify the trust that humans have when interacting with these systems.\nThe TAI Frameworks Project started modestly, aiming to provide a potential integration point for outputs from other TAI projects. It sought to inform the Navy in deployment decisions but has since grown and evolved. At the core of this endeavor are the six dimensions of trust in AI:\n\nExplainability: Making the logic and reasoning behind AI decisions transparent.\nSafety and Robustness: Ensuring that AI operates reliably under different conditions.\nNon-discrimination and Fairness: Guaranteeing that AI does not favor or prejudice against particular groups.\nPrivacy: Protecting individual’s information and rights.\nEnvironmental Sustainability: Recognizing the ecological footprint of AI systems.\nAccountability and Audibility: Holding AI and its developers responsible for decisions and actions.\n\nEach of these dimensions resonates with different stages of the AI lifecycle, including Data Collection, Data Preparation, Feature Extraction, Training, Testing, and Inference, and is aligned with the AI CI/CD of Development, Use, Analysis, and Re-Design.\n\n\n\nFigure 1: Conceptual Framework Principles for TrustedAI in the AI System Lifecycle\n\n\nReflection on the Framework Project\nAs we look back on the past years, we see an ever-evolving project that has adapted to the rapidly evolving landscape of Artificial Intelligence research and products. The integration of trust dimensions into our framework’s components, tools, and methodologies has shaped a solution aligned with the Navy’s mission. Furthermore, we have explored broader themes like ethical considerations, workforce development, and community involvement, highlighting the multifaceted nature of our approach to trusted AI. This is end of my first year as PI of the project after taking over from Prof. Jarek Nabrzyski who helmed the first year of the project with Prof. Chris Sweet and myself. The project was originally proposed by Prof. Ian Taylor, who went on leave from the CRC as the project was beginning that led to Prof. Jarek Nabrzyski to step in and provide leadership as PI. The direction of the project has changed since the acceptance of the initial proposal as a funded project, but many of the ideas and components of the proposal have remained. Work on evaluating and understanding the challanges and potential solutions to deploying AI technologies in the Navy, as well as capturing and understanding academic AI research outputs fell to Research Programmer Dr. James Sweet who spent time reviewing and understanding solutions that were being developed as open source projects and applied in industry to solve some of the challanges with “AIOps.” Additionally we had some help in creating some simple examples for using the Frameworks and starting the process of thinking about metadata for Trusted AI from Prof. Don Brower who came to the CRC last year from the Notre Dame Hesburgh Libraries. In year 2. we were joined by Prof. Paul Brenner, who has vast expeience with compute infrastructure and engineering in the military context. We were also joined by several UND undergraduate students Peter Ainsworth, Nicholas Clark and Daniel Weldon who created documentation, tutorials and “kicked the tires” on some of the Framework components in as part of the “AI Application” development process. We also had three students, Daniel Weldon, William Shephard, Samantha Nagel looking at existing cloud platforms and infrastructure in use by the DoD and attempting to containerize AI applications based on Pytorch to understand the complexity and barriers to AI applications in the DoD cloud.\n\n\n\n\n\n\nNote\n\n\n\nFramework components go here…\n\n\n\n\n\nFigure 2: Conceptual Framework Principles for TrustedAI in the AI System Lifecycle\n\n\nCommunity Engagement and Collaboration\n\n\n\n\n\n\nNote\n\n\n\nTPOCS, Other TAI projects, IU go here. Informing broader DoD workforce development efforts.\n\n\nOur journey has emphasized collaboration, dialogue, and shared learning. By leveraging existing tools and methodologies and engaging in broader community efforts, we have fostered an environment where knowledge is not just consumed but also contributed.\n\n\n\nFigure 3: TAI Research Themes (Husted 2022)\n\n\nLooking Ahead and Future Vision\nAs we move forward, our commitment to continuous alignment with trust dimensions and evolving methodologies remains steadfast. Our reflection on achievements and lessons learned will be a guiding light, illuminating the path towards future developments in trusted AI."
  },
  {
    "objectID": "posts/frameworks-reflection/index.html#building-a-foundation-for-trusted-ai",
    "href": "posts/frameworks-reflection/index.html#building-a-foundation-for-trusted-ai",
    "title": "Reflections on Trusted AI Frameworks after two years",
    "section": "2 Building a Foundation for Trusted AI",
    "text": "2 Building a Foundation for Trusted AI\n\n2.1 Software 1.0 vs Software 2.0: Building Trust from the Ground Up\nThe evolution from “Software 1.0” to “Software 2.0,” as popularized by Andrej Karpathy (Karpathy 2017), represents a paradigm shift in how we develop and deploy software systems.\n\nSoftware 1.0 is characterized by deterministic, programmed behavior where engineers write explicit code to define how a system should operate. It’s a method where rules are meticulously crafted, and the outcome is predictable, given a specific set of inputs.\nSoftware 2.0, in contrast, signifies learned behavior where models are trained on data to infer the underlying patterns. Unlike the rigid rules of Software 1.0, Software 2.0 can handle immense complexity, learning from examples to generalize and adapt to new situations. This ability to process and respond to intricate, multi-dimensional inputs sets Software 2.0 apart, opening doors to applications previously thought unfeasible.\n\nThe implications of Software 2.0 are profound for sectors that face highly complex and dynamic environments, such as those present in Navy operations. Where traditional rule-based systems may falter or become unwieldy, Software 2.0 offers the flexibility and adaptability to respond to ever-changing scenarios. Its capacity to absorb and interpret vast amounts of information in real-time provides a powerful tool in navigation, threat assessment, strategy formulation, and more. For the Navy, it enables a more resilient, intelligent, and agile approach to both strategic planning and operational execution.\nHowever, the promise of Software 2.0 doesn’t diminish the importance of Trusted “Software 1.0” as the foundation. Stability, predictability, and reliability remain essential, and the Trusted AI Frameworks Project recognizes the symbiotic relationship between these two paradigms. Trusted “Software 1.0” forms the stable base, while Trusted “Software 2.0” enables more dynamic and complex solutions.\nThe interplay between these two layers presents exciting opportunities and challenges, requiring rigorous validation, ethical considerations, and collaboration to ensure responsible deployment. It’s a multifaceted challenge that intertwines technology, ethics, collaboration, and vision, reflecting the complex reality of modern warfare and strategic planning.\n\n\n2.2 GitOps and AI Engineering: Collaborative Practices for Success\nIn the transition from “Software 1.0” to “Software 2.0,” where complexity and adaptability take center stage, the underlying practices that ensure quality and trust must also evolve. GitOps and collaborative environments like GitHub have emerged as essential tools in this transformation.\n\n2.2.1 What is GitOps?\nGitOps is a methodology that applies Git’s version control principles to the entire infrastructure and development lifecycle. By treating everything as code, including infrastructure and configuration, GitOps enables robust version control, audit trails, and collaboration. Changes are made in the Git repository, and automated systems ensure that the live environment aligns with the defined code state.\n\n\n2.2.2 Importance in Traditional and AI Engineering\n\nTraditional Software Engineering: In the context of “Software 1.0,” GitOps brings consistency, traceability, and collaboration. It provides a unified platform where the entire development team can work together, tracking changes, rolling back when necessary, and maintaining a clear history of modifications. This shared workspace enhances communication, accelerates development, and minimizes the risk of errors.\nAI Engineering: The shift to “Software 2.0” introduces new complexities and dependencies. GitOps adapts to these challenges by offering a flexible and responsive platform that can manage intricate workflows, diverse data sets, and multi-dimensional models. The collaborative nature of platforms like GitHub fosters cross-functional engagement, leveraging collective expertise to scrutinize, enhance, and validate AI models.\n\n\n\n2.2.3 Enhancing Trust and Quality\nBy embracing GitOps, projects can benefit from continuous integration, testing, and deployment. This CI/CD methodology promotes a culture of ongoing evaluation and refinement, where each change undergoes rigorous scrutiny. This process not only ensures quality but also builds trust, as stakeholders have transparent visibility into how decisions are made and how the software evolves.\nFor the Navy, where trust and reliability are paramount, this transparent, collaborative approach aligns with the ethos of responsibility and accountability. By implementing GitOps within the Trusted AI Frameworks Project, the partners foster an environment conducive to innovation, quality, and ethical considerations.\n\n\n\n2.3 CI/CD in Trusted AI: Ensuring Continuous Quality\nContinuous Integration/Continuous Deployment (CI/CD) is not a new concept in software development. Still, its application within the realm of AI and “Software 2.0” introduces unique opportunities and challenges. With AI’s rapid evolution and the growing prominence of foundation models and transfer learning, CI/CD becomes a vital component in maintaining quality, transparency, and trust.\n\n2.3.1 Applying CI/CD to AI “Software 2.0”\n\nFoundation Models: These are large-scale pre-trained models that serve as a base for specialized adaptations. By understanding intricate patterns in extensive data sets, foundation models can be fine-tuned for various tasks, enabling more rapid development and deployment.\nTransfer Learning Methodology: This approach leverages existing pre-trained models (often foundation models) and adapts them to specific tasks or domains. It allows for quicker iterations, conserves resources, and enhances performance on specialized tasks.\n\nIncorporating CI/CD into these AI practices ensures that as models are adapted and evolve, they continue to meet quality standards and align with ethical considerations. It promotes a transparent process, where every modification, every decision, can be traced and validated.\n\n\n2.3.2 Building Trust and Transparency\n\nRapid Integration and Evolution: With AI’s pace, a responsive CI/CD pipeline is essential. Models are constantly refined, adapted, and expanded. Ensuring that these changes are integrated smoothly, without compromising quality, is key to building trust.\nEthical Considerations: Transparency in the AI development process is more than a technical requirement; it’s an ethical obligation. A well-designed CI/CD process provides clear visibility into the workings of AI models, supporting responsible decision-making and accountability.\n\n\n\n2.3.3 “Data Engines” in AI CI/CD\nThe innovative concept of “Data Engines” in AI CI/CD refers to an intelligent, integrated system that orchestrates various stages of AI development, from data collection to model deployment.\n\nComponents and Functionality: A Data Engine incorporates real-time data ingestion, preprocessing, feature engineering, model training, evaluation, acceptance testing, and deployment. It serves as the backbone, ensuring that models adapt, learn, and evolve in alignment with real-world complexities and requirements.\nReal-World Example: Tesla’s Data Engine Pipeline: Tesla’s self-driving technology leverages a Data Engine that continually feeds information from vehicles on the road into its AI models. This pipeline manages vast quantities of data, refines features, adapts models, and deploys updates, all in an iterative and seamless manner. This example illustrates how Data Engines can handle the dynamism and complexity inherent in modern AI systems.\nChallenges and Solutions: Integrating a Data Engine within the AI CI/CD process is not without challenges. These may include ensuring data quality, scalability, adherence to ethical guidelines, and more. Innovative solutions and best practices, informed by real-world applications like Tesla’s, are instrumental in navigating these challenges successfully.\n\nThe integration of “Data Engines” within the AI CI/CD process marks a critical advancement in how we develop, deploy, and trust AI. For the Navy and the Trusted AI Frameworks Project, it signifies a forward-thinking approach that recognizes the dynamism of modern AI while steadfastly upholding quality and ethical principles.\n\n\n\n2.4 Ethical Considerations in Testing: Beyond Functionality\nAs we endeavor to create AI systems that interact with complex environments and human lives, the ethical implications of their behavior cannot be ignored. While traditional CI/CD GitOps environments are tailored to ensure functionality, quality, and performance, the nature of AI — particularly “Software 2.0” — necessitates a broader and more profound testing approach. This extends into the ethical realm, considering how the AI system behaves in various scenarios and aligns with societal values.\n\n2.4.1 Integrating Ethical Behavior Tests in AI CI/CD GitOps Environments\n\nAdapting Traditional Environments: Conventional CI/CD GitOps practices were not initially designed to handle the ethical complexities of AI systems. Integration of ethical behavior tests within these environments requires thoughtful adaptation and innovation to ensure that AI models align with ethical standards alongside technical requirements.\nEthical “Behavior Tests”: These are specialized tests designed to evaluate AI models in various scenarios, considering ethical principles like fairness, accountability, transparency, and safety. They are critical in building trust and ensuring that AI systems function not only effectively but ethically.\nChallenges with the “Data Engine” Concept: The real-time and continuous nature of Data Engines presents unique ethical challenges. How data is sourced, processed, and utilized within the engine has implications for privacy, bias, quality, and more. Ethical behavior tests must extend into this realm, evaluating the entire pipeline, including the Data Engine’s operations.\n\n\n\n2.4.2 Real-World Implications and the Navy’s Perspective\n\nImportance for the Navy and Marine Corps: For organizations like the Navy and Marine Corps, where AI systems may be deployed in life-critical and high-stakes environments, these ethical considerations are paramount. Ethical behavior tests must be part of the standard AI testing pipeline to ensure that AI deployments are not just functional but morally sound.\nCollaboration with Interdisciplinary Teams: Developing and implementing ethical behavior tests necessitates collaboration with ethicists, social scientists, legal experts, and other stakeholders. It’s a complex task that extends beyond technical engineering, involving a deeper understanding of human values and societal implications.\nBuilding a Comprehensive Ethical Framework: Alongside technical robustness, AI systems must be examined and validated for ethical alignment. This includes developing a comprehensive ethical framework within the CI/CD GitOps environment, guiding the design, development, and deployment of AI, keeping ethical considerations at the forefront.\n\n\n\n2.4.3 Conclusion: Building a Foundation of Trust and Responsibility in AI Development\nThe journey from “Software 1.0” to “Software 2.0” is more than a technological leap; it represents a shift in understanding and responsibility. As AI models grow more complex and intertwined with real-world applications, the demands on software engineering practices also evolve. The traditional foundations of trust and quality assurance in software development have had to adapt to new challenges posed by AI.\nIn this section, we’ve explored how concepts like GitOps and CI/CD practices must be reimagined to align with the unique characteristics of AI engineering. We delved into the creation of Data Engines, recognizing their potential in handling complex environments and real-world data but also acknowledging the ethical considerations they introduce.\nWe recognized that the ethical imperatives in AI go beyond mere functionality, requiring integrated ethical behavior tests, interdisciplinary collaboration, and a comprehensive ethical framework. This is especially pertinent for organizations like the Navy and Marine Corps, where the implications of AI behavior have profound and immediate impacts.\nThe synthesis of these concepts paints a picture of a rapidly changing landscape, one where the lines between traditional software engineering and AI development are blurring. It’s a landscape filled with opportunities, innovations, and new horizons, but also one that demands caution, foresight, and a steadfast commitment to ethical principles.\nThis foundation sets the stage for our subsequent exploration, where we’ll continue to dive into the intricacies of trusted AI, from data-centric methodologies to hardened containers and workforce development. As we journey through this complex terrain, the underlying theme of trust and ethical alignment remains our guiding star, illuminating the path toward responsible and successful AI deployment."
  },
  {
    "objectID": "posts/frameworks-reflection/index.html#managing-and-utilizing-data",
    "href": "posts/frameworks-reflection/index.html#managing-and-utilizing-data",
    "title": "Reflections on Trusted AI Frameworks after two years",
    "section": "3 Managing and Utilizing Data",
    "text": "3 Managing and Utilizing Data\n\n3.1 Data Centric AI for Trusted AI\nIn traditional AI development, much attention is often given to fine-tuning model architectures, experimenting with different layers, activation functions, and optimization algorithms. While this approach has driven many innovations, it may sometimes overlook the central role of data in AI success.\nA paradigm shift is emerging, championed by AI leaders like Andrew Ng through the Data Centric AI movement. This movement emphasizes the need to invest more in the quality and context of the data rather than continually fine-tuning the model architecture. The MIT Introduction to Data-Centric AI course provides a very good introduction to Data Centric AI methodology that the Frameworks aim to enable.\nThe logic behind this shift is profound yet straightforward. Models, irrespective of their complexity, are only as good as the data they learn from. By prioritizing data quality, encompassing aspects such as cleanliness, relevance, diversity, and contextual richness, AI systems can achieve better performance with potentially simpler architectures.\nThis data-centric approach aligns well with the requirements of organizations like the Navy and Marine Corps. In complex environments where variability and uncertainty are common, the depth and quality of data can make a significant difference. A well-annotated dataset that captures the intricacies of real-world scenarios allows AI models to learn more effectively and generalize better to unseen situations.\nFor the Trusted AI (TAI) Frameworks Project, a data-centric philosophy serves as an essential cornerstone, recognizing that robust data management practices can lead to more reliable, interpretable, and trusted AI systems. This perspective does not diminish the value of innovative modeling but places it in the context of a balanced and well-considered AI development strategy, where data and model work in harmony.\nThe data-centric approach is not just a technical reorientation; it’s a recalibration of AI development’s very essence, placing data at the heart of the innovation process. The implications for trusted AI are profound, offering a path to AI systems that are not only highly capable but also aligned with the nuanced requirements and ethical considerations that define the evolving landscape of AI in military and civil applications.\n\n\n\nFigure 4: Trusted AI Needs Trusted Data as pointed out by: “Flawed Data”\n\n\n\n\n3.2 DoD Data Strategy: Aligning with National Defense Goal\nThe Department of Defense’s (DoD) Data Strategy emphasizes the critical role that data plays in achieving the goals of the National Defense Strategy:\n\n\n\n\n\n\n(“DoD Data Strategy: Unleashing Data to Advance the National Defense Strategy” 2020)\n\n\n\n\nMake Data Visible – Consumers can locate the needed data.\nMake Data Accessible – Consumers can retrieve the data.\nMake Data Understandable – Consumers can recognize the content, context, and applicability.\nMake Data Linked – Consumers can exploit data elements through innate relationships.\nMake Data Trustworthy – Consumers can be confident in all aspects of data for decision-making.\nMake Data Interoperable – Consumers have a common representation comprehension of data.\nMake Data Secure – Consumers know that data is protected from unauthorized use/manipulation.\n\n\n\nThe TAI Frameworks Project highlights its commitment to a data-centric approach to trusted AI. Each goal resonates with our overarching mission:\n\nVisible: Ensuring that data is discoverable by those who need it fosters transparency and helps build a foundation of trust in AI systems.\nAccessible: Making data readily available to authorized users enhances the efficiency and effectiveness of AI, providing the right information at the right time.\nUnderstandable: Clear documentation and metadata contribute to the explainability of AI, a core dimension of trust.\nLinked: Connecting related data sets facilitates more coherent AI analysis, ensuring robustness and reliability in decision-making.\nTrustworthy: Maintaining the integrity and quality of data ensures that AI systems are dependable, echoing the Trustworthy dimension in our trust framework.\nInteroperable: Enabling data to be used across different systems and platforms fosters collaboration and integration, key aspects of our community engagement efforts.\nSecure: Implementing strong data security measures safeguards privacy and aligns with the ethical considerations central to trusted AI.\n\nThis alignment with the DoD’s data goals underscores the importance of a data-centric approach in our work. By recognizing data as a strategic asset and prioritizing these seven goals, we are nurturing an environment where trusted AI can flourish, complementing the “Data-Centric AI for Trusted AI” section that follows.\n\n\n3.3 Selection of Data Version Control (DVC) for Tracking Data and Experiments\nManaging and tracking data and experiments is a critical task in the AI development lifecycle. Traditional version control systems are well-suited to handle code, but they often fall short when dealing with the unique requirements of large-scale data. In the context of AI, this limitation can hinder the collaboration and reproducibility that are vital to building trusted systems.\nData Version Control (DVC) emerges as a compelling solution to address these challenges, and its selection in the Trusted AI (TAI) Frameworks Project highlights its inherent advantages. Here’s an exploration of why DVC stands apart from other data management systems:\n\nIntegration with GitOps: DVC extends Git’s functionality to handle large data files without storing them in the Git repository. It uses pointers in the repository, keeping the actual data in a remote location. This approach aligns with GitOps practices, allowing developers to manage data with the same tools and workflows they use for code, promoting a more streamlined and unified process.\nOpen Source Nature: DVC is an open-source project, fostering a community-driven approach that aligns with the values of collaboration, transparency, and accessibility. This aspect encourages active participation from a wide range of contributors, leading to rapid innovations and responsive support.\nCompatibility with CI/CD and GitHub: DVC’s architecture is designed to work seamlessly within CI/CD pipelines, and its compatibility with GitHub Actions enables automated workflows that cover everything from data preprocessing to model training and evaluation. This integration ensures that data handling is not a separate silo but an integral part of the continuous integration, testing, and deployment processes.\nData Tracking and Experiment Versioning: Unlike traditional data management systems, DVC provides robust capabilities to track changes in data and experiments, much like how code is versioned. It enables the ability to revert to previous data states, compare different data versions, and align data changes with specific code revisions. This ensures that every step in the AI development process can be audited and replicated, contributing to greater trust and transparency.\nFacilitating Collaboration: DVC’s decentralized structure and remote storage options enable teams to share and access data without overwhelming the repository. This encourages collaboration, even in distributed environments, ensuring that everyone can work on the same data version and maintain consistency across the development lifecycle.\nFlexible Storage Options: DVC offers flexibility in storing data, supporting various remote storage backends like S3, Azure Blob, Google Cloud Storage, among others. This flexibility aligns with different organizational needs and preferences, ensuring that data management is adaptable to various scenarios.\n\nThe selection of DVC in the TAI Frameworks Project represents a thoughtful alignment with the principles of trusted AI, GitOps, and open-source collaboration. By bridging the gap between traditional software engineering practices and the unique demands of AI data management, DVC supports the creation of AI systems that are not only performant but also transparent, replicable, and ethical.\n\n\n\n\n\n\nNote\n\n\n\nAdd information about MLFlow and other tools evaluated in this section.\n\n\n\n\n3.4 Ethical Considerations and Trust in Data Management: The Navy’s Mission and Guiding Principles\nIn alignment with the Navy’s strategic goals and the specific executive orders governing the Department of Defense (DoD), ethical considerations and trust in data management hold a central place in the Trusted AI Frameworks Project. By prioritizing metadata, adhering to the FAIR (Findable, Accessible, Interoperable, and Reusable) and CARE (Collective Benefit, Authority to Control, Responsibility, and Ethics) principles, the Navy reinforces its commitment to mission-critical values of transparency, integrity, and accountability. It should be noted that application of FAIR and CARE principles are separate from the notion of “Open Data” and doesn’t invalidate the use of proper access and authorization controls.\n\n3.4.1 Metadata and Its Role in Ethical Data Management\n\nTransparency for Mission Success: Metadata supports the Navy’s operational efficiency by providing clarity on data origins, context, and constraints, thereby enabling informed decision-making.\nCompliance with Executive Orders: Metadata ensures alignment with legal and regulatory requirements, such as the DOD AI Ethical Principles (Department of Defense 2020), which mandates specific data handling protocols within the DoD and specifically:\n\n\n\n\n\n\n\n(Department of Defense 2020)\n\n\n\n\nResponsible. DoD personnel will exercise appropriate levels of judgment and care, while remaining responsible for the development, deployment, and use of AI capabilities.\nEquitable. The Department will take deliberate steps to minimize unintended bias in AI capabilities.\nTraceable. The Department’s AI capabilities will be developed and deployed such that relevant personnel possess an appropriate understanding of the technology, development processes, and operational methods applicable to AI capabilities, including with transparent and auditable methodologies, data sources, and design procedure and documentation.\nReliable. The Department’s AI capabilities will have explicit, well-defined uses, and the safety, security, and effectiveness of such capabilities will be subject to testing and assurance within those defined uses across their entire life-cycles.\nGovernable. The Department will design and engineer AI capabilities to fulfill their intended functions while possessing the ability to detect and avoid unintended consequences, and the ability to disengage or deactivate deployed systems that demonstrate unintended behavior.\n\n\n\n\n\n3.4.2 The FAIR Principles: Ensuring Naval Readiness\n\nFindable & Accessible: Easy discovery and retrieval of data accelerate the Navy’s response time and readiness.\nInteroperable & Reusable: Standards for data compatibility facilitate cross-departmental collaboration and resource optimization, reflecting the Navy’s emphasis on agility and resilience.\n\n\n\n3.4.3 CARE Principles: Upholding the Navy’s Ethical Values\n\nCollective Benefit: Data practices that align with community interests echo the Navy’s mission to serve and protect the nation.\nAuthority to Control & Responsibility: Respecting the rights of data providers and upholding stewardship responsibilities mirror the Navy’s core values of honor and commitment.\nEthics: Conducting data activities with integrity aligns with the Navy’s ethical framework and compliance with relevant executive orders, such as Executive Order YYYY on Ethical Considerations in the DoD.\n\n\n\n3.4.4 Conclusion: Setting a Standard for Naval Excellence\nThe Trusted AI Frameworks Project’s alignment with metadata, FAIR, and CARE principles reflects the Navy’s dedication to excellence, ethical leadership, and mission success. By embracing these principles, the Navy not only fosters trust and transparency within its ranks but also sets a precedent for responsible data handling that extends to its partnerships with other military branches, academia, and industry.\n\n\n\n3.5 Future Perspectives and Conclusion: The Data-Driven Path to Trusted AI in Year 3 and Beyond\nAs the Trusted AI Frameworks Project enters its third year, the focus on data-centric approaches continues to sharpen, reflecting the broader trend in AI development. In this complex and ever-evolving landscape, the partnership between Indiana University, University of Notre Dame, CRANE Naval Surface Warfare Center, and other collaborators is more crucial than ever. Here, we explore what lies ahead and conclude with insights that underline the strategic importance of data management for the Navy and Marine Corps.\n\n3.5.1 Future Perspectives: Data Management in Year 3 and Beyond\n\nContinued Emphasis on Data Quality: Leveraging the momentum of the data-centric AI movement, the next phase will focus on further refining data quality and context.\nAdoption of Emerging Technologies: Integration with cutting-edge tools and methodologies will enhance agility in data handling.\nStrengthened Collaboration with Open Source Communities: The project’s commitment to using and contributing to open-source tools will continue to foster innovation and collaboration.\n\n\n\n3.5.2 Navigating Ethical Considerations and Regulatory Compliance\n\nAlignment with Executive Orders: Ongoing alignment with legal and regulatory requirements will be paramount.\nEnhanced Ethical Oversight: A robust ethical framework that resonates with FAIR and CARE principles will guide the data management efforts.\n\n\n\n3.5.3 Implications for the Navy and Marine Corps\n\nStrategic Agility: Improved data management facilitates faster and more informed decision-making, crucial for naval readiness.\nEthical Leadership: Upholding the highest standards of integrity aligns with the Navy’s core values and enhances trust.\n\n\n\n3.5.4 Conclusion: A Unified Path Forward\nThe data-driven journey of the Trusted AI Frameworks Project illuminates the vital role of data in enhancing the trustworthiness and success of AI deployment within the Navy and Marine Corps. With a vision rooted in collaboration, ethical excellence, and technological innovation, the path forward promises to be an exciting and transformative adventure. Together, academia, industry, and the military will continue to forge a data-centric future that not only serves the needs of the present but anticipates the challenges and opportunities of tomorrow."
  },
  {
    "objectID": "posts/frameworks-reflection/index.html#deployment-and-provisioning",
    "href": "posts/frameworks-reflection/index.html#deployment-and-provisioning",
    "title": "Reflections on Trusted AI Frameworks after two years",
    "section": "4 Deployment and Provisioning",
    "text": "4 Deployment and Provisioning\n\n4.1 Introduction: The Challenge of Reproducibility and Integration\nTrusted AI involves complex and interdependent components that often originate from disparate academic and open-source projects. This complexity presents unique challenges in ensuring reproducibility and seamless integration of both “software 1.0” and “software 2.0.” The need for a robust framework that addresses these challenges has never been more pertinent. The project’s emphasis on deployment and provisioning aims to build a bridge between the theoretical foundations of AI and the practicalities of its application within a secure and trustworthy framework.\nCertainly! Here’s a revised version of the “DevOps and Computational Environments” section that provides more context and explanations for those who may not be familiar with these technologies:\n\n\n4.2 DevOps and Computational Environments: Bridging Development and Operations\nThe journey of Trusted AI doesn’t end with the creation of intelligent algorithms but extends into the realms of deployment and operational environments. This nexus between development and operations is known as DevOps, a practice that enhances efficiency, reliability, and maintainability. In the context of Trusted AI, several technologies and practices have been leveraged, each serving a specific role:\n\nPython PDM: As Python is one of the most prevalent languages for AI development, tools like Python’s Package Dependency Management (PDM) are essential. PDM helps manage dependencies and package versions, ensuring consistent environments across development, testing, and deployment stages.\nConda/Mamba: These are package managers specifically designed for scientific computing and data science, including AI. They allow for the creation of isolated environments, simplifying the process of managing complex dependencies.\nNix Containers: Nix takes reproducibility to the next level. Unlike conventional package managers, Nix ensures that the exact versions of all dependencies are explicitly defined, creating a consistent and reproducible environment across different stages and machines. This is crucial for both “software 1.0” and “software 2.0,” where inconsistencies in environments can lead to unpredictable behaviors.\nTemplates and Devcontainers: By using templates and development containers (devcontainers), the project has streamlined the setup process, making it easier for developers to replicate environments. This ensures that what works on one developer’s machine will work on another’s, alleviating the infamous “it works on my machine” problem.\n\nThese tools and practices collectively create a seamless and reliable process of moving AI models and applications from development to production. They recognize the complexity of AI development and provide solutions tailored to handle these challenges. The selection of these specific technologies within the Trusted AI project is a strategic alignment with the unique needs of AI, reflecting an understanding of the underlying challenges and opportunities that AI presents.\nBy connecting these dots and creating an ecosystem where development and operations converge, DevOps in the Trusted AI context represents a key pillar in the project’s strategy. It enables not just the creation but also the deployment and maintenance of AI solutions that can be trusted, validated, and reproduced, meeting the stringent standards required by the Navy and other critical stakeholders.\n\n\n4.3 Software Bill of Materials (SBoM): A Comprehensive Inventory for Trust\nIn any sophisticated piece of technology, understanding the components that make up the system is essential for security, quality, and compliance. The Software Bill of Materials (SBoM) serves this very purpose, offering a comprehensive inventory of all software components in a system, akin to a parts list in physical manufacturing.\nThe SBoM is especially critical in the context of Trusted AI for the Navy, aligning with Federal Government requirements and standards for procurement. Here’s a breakdown of the key elements and why they matter:\n\nWhat is an SBoM?: An SBoM details the components, libraries, and modules within a software product. It includes the exact versions, sources, licenses, and dependencies, offering a transparent and verifiable snapshot of the software’s composition.\nSPDX (Software Package Data Exchange): Developed by the Linux Foundation, SPDX provides a standardized format for sharing SBoM data across different tools and systems. It enables consistency and interoperability, supporting transparency and trust in software procurement and deployment.\nSPDX Version 3: Currently under final revision, this new version of SPDX expands its capabilities to include specifications for AI Bill of Materials. It recognizes the unique attributes of AI and machine learning models, accommodating their complex dependencies and configurations.\nJSON-LD Vocabulary: SPDX’s vocabulary released as JSON-LD (JSON Linked Data) provides a machine-readable format that facilitates the automatic processing of SBoM data. It streamlines integration with existing systems and tools, fostering collaboration and automation in AI deployment.\nRelevance to the Navy and Federal Requirements: Compliance with SBoM requirements is essential for the Department of Defense (DoD) and other federal agencies. It aligns with executive orders and guidelines governing software procurement, security, and transparency, making it a crucial component of the Trusted AI project’s success.\n\nBy embracing the SBoM, the Trusted AI project is leading the way in responsible and transparent software development and deployment. The use of SPDX and alignment with federal standards ensures that the AI models and applications meet rigorous security and quality benchmarks. This not only promotes trust within the Navy but also sets a precedent for AI projects in various sectors, demonstrating the importance of detailed, transparent documentation in building robust and reliable AI systems.\nThe alignment of the SBoM within the Trusted AI framework emphasizes the commitment to excellence and adherence to regulations, reinforcing the project’s mission to deliver AI solutions that can be confidently deployed and maintained within critical environments.\n\n\n4.4 Hardened Containers, Military CI/CD through IronBank, Cloud One, and Platform One\nThe evolving technological landscape has necessitated a robust infrastructure for deploying software. Within the military context, especially in the Navy, trust, security, and efficiency are paramount. IronBank, Cloud One, and Platform One are initiatives that provide trusted access to cloud services and deployments (e.g., AWS, Azure) and create a streamlined pathway for the adoption of AI and other advanced technologies.\n\n4.4.1 Introduction to IronBank, Cloud One, and Platform One\n\nIronBank: A digital repository that offers containerized software accredited to run in Department of Defense (DoD) environments.\nCloud One: A set of cloud services designed to enable rapid and secure development within the DoD.\nPlatform One: A DoD-wide initiative that provides containerized software solutions, incorporating DevSecOps principles for continuous integration and deployment.\n\nThese platforms reduce the barriers to entry, ensuring a standardized approach to software development and deployment, all while aligning with military guidelines and requirements.\n\n\n4.4.2 AI-Centric Workflows and Challenges:\nIntegrating AI within these platforms presents unique challenges that diverge from traditional software deployments: - Data Management: AI’s unique data needs must be met through robust strategies, accommodating real-time feeds and scalable workflows. - Hardware Needs: AI applications often necessitate specialized hardware such as GPUs and accelerators, requiring containerization techniques that are hardware-agnostic. - Device Driver Complexity: Managing device drivers within Hardened Containers can be particularly intricate, leading to potential security vulnerabilities if misconfigured.\n\n\n4.4.3 Solutions and Alignment with Military Objectives:\nTo tackle these challenges, the combined ecosystem must: - Enhance data management to align with AI’s distinct requirements. - Adopt standardized device driver management and containerization strategies. - Ensure complete compliance with the Linux Foundation’s SPDX standards and federal procurement needs.\nBy integrating AI-centric workflows and aligning with essential hardware and security needs, IronBank, Cloud One, and Platform One offer a strong foundation for the military’s future AI adoption.\nIn the ever-changing technological landscape, the military’s approach to deployment and provisioning must evolve. The alignment of IronBank, Cloud One, and Platform One with AI’s unique workflows, specialized hardware, and secure device driver management sets a robust foundation for future innovation. These platforms not only align with the Navy’s mission but also pave the way for broader AI integration within the military domain, enhancing both trust and functionality.\n\n\n4.4.4 Conclusion: Building Trust through Secure Deployment\nThe deployment and provisioning of AI within the military environment are complex tasks laden with unique challenges. The emergence of hardened containers and specific initiatives like IronBank, Cloud One, and Platform One provides a pathway to integrating AI into the military domain. However, it’s the standardization through tools like Python PDM, Conda/Mamba, and Nix Container that builds a foundation of trust and robustness.\nBy embracing standardized tools, aligning them with military needs, and integrating them with existing trusted platforms, the Navy is poised to leverage the full potential of AI. The overarching strategy must be one of collaboration, security, and forward-thinking innovation, ensuring that the deployment of AI aligns with the broader goals of efficiency, transparency, and trust within the Navy’s mission.\nThe road ahead is promising, with these strategic initiatives paving the way for AI’s broader integration into the military domain. The combination of standardization, compliance, and alignment with military needs ensures a path toward Trusted AI that is both innovative and grounded in secure principles."
  },
  {
    "objectID": "posts/frameworks-reflection/index.html#development-practices-and-methodologies",
    "href": "posts/frameworks-reflection/index.html#development-practices-and-methodologies",
    "title": "Reflections on Trusted AI Frameworks after two years",
    "section": "5 Development Practices and Methodologies",
    "text": "5 Development Practices and Methodologies\n\n5.1 Introduction: The Modern Development Landscape\nThe field of software development has seen a series of paradigm shifts, each reflecting the evolving demands of technology and society. From the traditional waterfall model to the agile development framework, the focus has continuously shifted towards more iterative, responsive, and collaborative practices. Within this changing landscape, the concept of “Literate Programming” was introduced by Donald Knuth in the 1980s, emphasizing the importance of explaining code in a human-readable narrative. Knuth’s vision foresaw a future where programming is not just a mechanical process but a craft interwoven with storytelling and documentation.\nWith the rise of AI, this vision has been pushed further, enabling a new era of collaboration between human programmers and AI “Co-Pilots.” AI-driven assistance in software development is becoming a reality, allowing for more effective and efficient code writing, debugging, and optimization. Literate programming can play a vital role in this context, as it emphasizes clarity, explanation, and a structured narrative that both human developers and AI models can interpret and build upon.\nIn the realm of AI development, tools like Jupyter Notebooks foster a dynamic environment that bridges the gap between exploration and production, aligning closely with literate programming concepts. AI Co-Pilots can leverage this environment, utilizing the human-friendly documentation to better understand developer intent and provide more relevant assistance.\nThe integration of Language Model based generative models, like large language models, opens up new horizons for collaborative and adaptive software development. These models can navigate the intricate landscape of literate programming, making sense of both code and context, enhancing their ability to assist humans in complex software development tasks.\nAgile principles resonate strongly in this context, mirroring the exploratory nature of AI development. Just as agile methodologies prioritize adaptability and collaboration, modern AI practices encourage experimentation, quick iterations, and a seamless transition from exploration to deployment. The subsequent sections delve into specific practices and tools that exemplify this convergence of agility, literacy, and exploratory programming, demonstrating their relevance and potential within the framework of Trusted AI.\n\n\n5.2 Quarto: Bridging the Gap Between Literacy and Code\n\n5.2.1 Introduction to Quarto\nQuarto is a pioneering solution in the field of literate programming that seamlessly integrates human-readable text with executable code. It builds on the vision of “Literate Programming” put forth by Donald Knuth, which emphasizes the importance of making source code as understandable as ordinary written prose.\n\n\n5.2.2 Key Features of Quarto\n\nHuman-Readable Documentation: Quarto allows developers to intertwine code with descriptive narratives, diagrams, and mathematical notation. This approach fosters comprehension, collaboration, and maintainability, making it particularly well-suited for complex AI projects.\nInteroperability with Jupyter Notebooks: By supporting Jupyter Notebook integration, Quarto enables an exploratory programming environment. Researchers and engineers can craft, test, and iterate on code in an interactive and visual manner.\nFlexible Output Formats: Quarto documents can be rendered into various formats such as HTML, PDF, and Word, catering to diverse presentation and distribution needs.\nReproducibility: Code chunks and outputs within Quarto documents are readily reproducible. This aids in verification and validation, essential qualities in Trusted AI.\n\n\n\n5.2.3 Quarto in the Context of AI Development\nIn the rapidly evolving landscape of AI, where complexity can easily spiral out of control, Quarto provides a structured and transparent way to document and develop models. The following aspects make it a particularly fitting choice:\n\nTransparency: Literate programming through Quarto ensures that the code’s logic and decision-making are clearly articulated, fostering trust in the AI system.\nCollaboration: By bridging the gap between code and prose, Quarto facilitates collaboration among multidisciplinary teams, including data scientists, engineers, domain experts, and stakeholders.\nCompliance: In contexts like the Navy, where regulatory compliance and ethical considerations are paramount, Quarto’s approach to documentation can aid in meeting these requirements.\n\n\n\n5.2.4 Conclusion\nQuarto represents a leap forward in aligning software development practices with human cognition and communication. Its ability to bridge literacy and code makes it an indispensable tool in the era of “Software 2.0.” By fostering clear understanding, collaboration, and reproducibility, Quarto contributes to the overarching goal of building Trusted AI systems, aligning perfectly with the needs and values of modern AI development, including military applications.\n\n\n\n5.3 nbdev: Extending Literacy with Quarto Integration\n\n5.3.1 Introduction to nbdev\nnbdev is an open-source development environment that builds upon Jupyter Notebooks and extends the principles of literate programming. It particularly leverages the Quarto framework to create a seamless and productive development experience. By combining code, prose, and visualizations into a single environment, nbdev enables a more transparent and collaborative approach to software development.\n\n\n5.3.2 Key Features of nbdev\n\nQuarto Integration: By incorporating Quarto, nbdev fosters a rich literate programming environment where code is documented and explained in human-readable form.\nInteractive Development: Developers can write code, run experiments, and analyze results all within the same Jupyter Notebook, promoting a more exploratory and iterative process.\nAutomated Code Export: Code written in nbdev’s notebooks can be automatically converted to traditional script files, facilitating integration with existing codebases and deployment pipelines.\nTesting Integration: nbdev supports continuous integration and testing within the notebook environment, aligning with modern software quality assurance practices.\nCollaboration Tools: With built-in support for Git and GitHub, nbdev streamlines collaboration and version control, bridging the gap between data scientists, developers, and other stakeholders.\n\n\n\n5.3.3 nbdev in the Context of AI Development\nThe flexibility and transparency offered by nbdev are especially significant in AI development. Here’s how:\n\nComplex Model Documentation: AI models are inherently complex and require detailed explanation. Nbdev allows for the clear documentation of both algorithms and the reasoning behind design choices.\nRapid Prototyping: The interactive nature of nbdev enables quicker experimentation and refinement of models, a crucial aspect of AI development.\nReproducibility: By enforcing a structured approach, nbdev ensures that models are more reproducible, enhancing trust and reliability.\nAlignment with Software 2.0 Principles: The nbdev framework aligns well with the “Software 2.0” paradigm, where learned behaviors and model training are central. It supports the rapid evolution and integration needed in the AI landscape.\n\nnbdev, with its integration of Quarto, provides a cohesive and advanced environment for literate programming. Its synergy with the principles of modern AI development makes it an essential tool for achieving transparency, collaboration, and efficiency. In contexts like the military, where understanding, trust, and compliance are key, nbdev offers a pathway to create more accessible and trustworthy AI systems.\n\n\n\n5.4 Fast.ai: A Layered Approach to AI Development\n\n5.4.1 Introduction to Fast.ai\nFast.ai is an open-source deep learning library that simplifies the process of building and training deep learning models. Its core philosophy is to offer a layered approach, providing APIs at different levels of abstraction. This flexibility empowers developers, data scientists, and researchers to engage with AI across various complexities, from high-level applications to low-level customization.\n\n\n5.4.2 Fast.ai’s Layered Approach\n\nApplications Layer: This layer offers pre-built solutions for common tasks like image classification, natural language processing, tabular data analysis, and collaborative filtering. It is designed for ease of use, allowing non-specialists to build and deploy models with minimal code.\nHigh-Level API: For those who need more control and customization, the high-level API provides an intuitive interface to design and experiment with more complex models. It strikes a balance between simplicity and power, suitable for intermediate-level developers.\nMid-Level API: The mid-level API introduces more granular control and allows for the creation of custom models and data pipelines. It’s designed for advanced users who wish to dive deeper into the underlying mechanics of their models.\nLow-Level API: At the lowest layer, developers have direct access to underlying PyTorch constructs, facilitating experimentation and the development of entirely novel architectures. This layer is aimed at research scientists and cutting-edge practitioners.\n\n\n\n5.4.3 Literate Programming and nbdev Integration\nFast.ai not only encourages a more accessible approach to AI but also promotes literate programming principles. The library itself was developed using nbdev, thereby enhancing its transparency, documentation, and collaboration.\n\nTransparent Development: Literate programming within nbdev enables Fast.ai’s code, design rationale, and experiments to be clearly documented and shared.\nInteroperability: Fast.ai’s integration with nbdev ensures smooth collaboration between different stakeholders in the AI development process, including data scientists, engineers, and domain experts.\nRapid Prototyping and Experimentation: Leveraging the exploratory nature of nbdev, Fast.ai facilitates iterative experimentation and refinement, vital for AI research and development.\n\n\n\n5.4.4 Relevance to the Navy and Broader Defense Community\nFast.ai’s layered approach is particularly relevant to the Navy and other defense institutions. Here’s why:\n\nScalability: Fast.ai’s structure allows for quick prototyping and scaling, accommodating both straightforward applications and sophisticated custom solutions.\nEducation and Workforce Development: The approachability of Fast.ai makes it an excellent tool for education and training, aligning with initiatives like Scalable Asymmetric Lifecycle Engagement (SCALE) managed by Purdue University.\nTrust and Compliance: The transparency and reproducibility afforded by Fast.ai, especially in combination with nbdev, align well with the principles of Trusted AI, a critical consideration for military applications.\n\nFast.ai represents a significant advancement in making AI accessible, understandable, and manageable. Its layered approach, combined with literate programming principles, contributes to a more transparent and collaborative development process. In the context of defense and national security, Fast.ai offers a pathway to harness the power of AI effectively, ethically, and responsively.\n\n\n\n5.5 Conclusion: The Evolution of Development Practices in Trusted AI\nThe landscape of software development, particularly in the realm of AI, is undergoing a radical transformation. As we have explored in this section, the integration of literate programming, layered APIs, and open-source tools like Quarto, nbdev, and Fast.ai represent a shift towards more transparent, collaborative, and efficient development practices. These shifts are not just technological but cultural, redefining how developers, data scientists, engineers, and domain experts collaborate to create intelligent systems.\n\n5.5.1 The Role of AI Co-Pilots\nA promising frontier in this evolution is the integration of AI itself into the development and deployment process. AI Co-Pilots, or intelligent assistants, are emerging as key players in this new paradigm.\n\nSecurity Oversight: AI Co-Pilots can continuously monitor code for potential security vulnerabilities, offering real-time insights and even automated fixes, aligning with Trusted AI principles.\nCI/CD Integration: By embedding intelligence into the Continuous Integration/Continuous Deployment pipeline, AI Co-Pilots can optimize testing, deployment, and scaling, adapting to the unique demands of AI “Software 2.0.”\nCode Quality Control: AI-driven code review can ensure adherence to coding standards, improve code quality, and reduce human error, fostering more robust and maintainable systems.\nAdaptive Learning and Enhancement: Leveraging real-world data, AI Co-Pilots can learn and evolve, offering personalized assistance and driving continuous improvement in development practices.\n\n\n\n5.5.2 Relevance to the Navy and Defense Community\nThe Navy, and the defense community at large, stand to benefit profoundly from these advancements. The integration of AI Co-Pilots can enhance the security, efficiency, and agility of development processes, key attributes in an ever-changing and complex environment.\n\nMission Readiness: Faster, more robust development practices mean quicker deployment of critical AI systems, enhancing responsiveness and strategic capabilities.\nEthical Compliance: Automated oversight aligned with ethical guidelines ensures that AI systems adhere to the necessary standards, fostering trust and accountability.\nWorkforce Development: AI-driven tools offer opportunities for training and upskilling, aligning with initiatives like the Scalable Asymmetric Lifecycle Engagement (SCALE) managed by Purdue University, enhancing both semiconductor and AI workforce development.\n\n\n\n5.5.3 Looking Ahead\nThe integration of AI into development practices represents a significant step towards a future where intelligent systems are not just the products but active participants in the development lifecycle. This shift promises to enhance not only the efficiency and quality of development but also the ethical alignment and trustworthiness of AI systems.\nIn an era where complexity is the norm, and the stakes are high, these advancements offer a path towards a more resilient, responsive, and responsible approach to AI. The convergence of AI and human expertise is not just a technical evolution but a philosophical one, reflecting a deeper understanding of the interconnectedness of technology, ethics, and humanity.\n\n\n\n5.6 Broader Challenges and Opportunities\nThe quest for Trusted AI is not merely a technical challenge; it’s a complex endeavor that intersects with various domains, including ethics, community involvement, workforce development, interdisciplinary collaboration, and more. This section highlights some of these broader considerations, emphasizing the multifaceted nature of Trusted AI.\n\n5.6.1 Workforce Development\n\nSkills Gap: With the rapid advancement of AI technologies, there is a growing need for professionals with specialized skills. Addressing this gap requires tailored training and educational programs.\nInitiatives like SCALE: Managed by Purdue University and focused on semiconductor and AI workforce development, SCALE represents a model for public-private collaboration in nurturing talent.\nInclusion and Diversity: Building a workforce that reflects diverse perspectives is essential for developing AI systems that are equitable and unbiased.\n\n\n\n5.6.2 Ethical Considerations\n\nTransparency and Accountability: Ensuring that AI systems are transparent and accountable is fundamental to building trust.\nEthical Testing: Integrating ethical “behavior tests” within AI testing pipelines, as highlighted in CI/CD and GitOps environments, is vital to align AI with societal values.\nRegulatory Compliance: Adherence to executive orders, regulations, and guidelines governing DoD and Navy’s use of AI is crucial for legal and ethical operation.\n\n\n\n5.6.3 Community and Open Source Involvement\n\nCollaboration with Open Source Tools: Leveraging and contributing to open-source tools fosters a collaborative and innovative ecosystem.\nEngagement with the Broader Community: Collaborating with academia, industry, and other stakeholders can accelerate progress and ensure alignment with broader societal goals.\nPromoting Open Standards: Encouraging the use of open standards like Linux Foundation SPDX helps in creating a universal language and facilitating collaboration across different entities.\n\n\n\n5.6.4 Interdisciplinary Collaboration\n\nBridging Silos: Trusted AI requires the fusion of expertise from computer science, ethics, law, social sciences, and more. Encouraging interdisciplinary collaboration breaks down barriers and fosters holistic solutions.\nInnovation through Diversity of Thought: By integrating diverse perspectives, new pathways to innovation are uncovered, driving creativity and resilience in the development of AI systems.\n\n\n\n5.6.5 User Experience and Human-AI Interaction\n\nHuman-Centered Design: Emphasizing the end-user’s needs and expectations ensures that AI systems are user-friendly, accessible, and effective.\nEvolving Interaction Models: As Trusted AI projects mature, new paradigms of Human-AI Interaction (HAI) will emerge, necessitating ongoing research and adaptation.\n\n\n\n\n5.7 Conclusion: A Holistic Approach to Trusted AI\nTrusted AI is not a monolithic entity but a complex, dynamic field that intersects with various disciplines and societal considerations. By embracing these broader challenges and opportunities, we pave the way for AI systems that are not just intelligent but responsible, ethical, and aligned with human values and needs.\nThe integration of these various facets into the Trusted AI framework contributes to a comprehensive and sustainable approach, one that recognizes the interconnectedness of technology with the broader fabric of society. The journey towards Trusted AI is a collective endeavor, one that requires collaboration, empathy, foresight, and a deep commitment to ethical principles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Success Factors: Engineering Trust in Deployments",
    "section": "",
    "text": "Welcome To AI Success Factors: Engineering Trust in Deployments\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nCharles F. Vardeman II\n\n\n\n\n\n\n  \n\n\n\n\nReflections on Trusted AI Frameworks after two years\n\n\n\n\n\n\n\nTrusted AI\n\n\nAI Frameworks\n\n\nAI Engineering\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nCharles Vardeman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/trust-causal/index.html#towards-trustworthy-and-aligned-machine-learning-a-data-centric-survey-with-causality-perspectives",
    "href": "slides/trust-causal/index.html#towards-trustworthy-and-aligned-machine-learning-a-data-centric-survey-with-causality-perspectives",
    "title": "Trust and Causal Reasoning",
    "section": "Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives",
    "text": "Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives\n\n\nLearn more: ArXiv Paper"
  },
  {
    "objectID": "slides/trust-causal/index.html#towards-trustworthy-and-aligned-ml-website",
    "href": "slides/trust-causal/index.html#towards-trustworthy-and-aligned-ml-website",
    "title": "Trust and Causal Reasoning",
    "section": "Towards Trustworthy and Aligned ML Website",
    "text": "Towards Trustworthy and Aligned ML Website\n\n\nTowards Trustworthy and Aligned ML"
  },
  {
    "objectID": "slides/trust-causal/index.html#towards-trustworthy-and-aligned-machine-learning-a-data-centric-survey-with-causality-perspectives-1",
    "href": "slides/trust-causal/index.html#towards-trustworthy-and-aligned-machine-learning-a-data-centric-survey-with-causality-perspectives-1",
    "title": "Trust and Causal Reasoning",
    "section": "Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives",
    "text": "Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives\n\n\nLearn more: ArXiv Paper"
  },
  {
    "objectID": "slides/trust-causal/index.html#causal-reasoning",
    "href": "slides/trust-causal/index.html#causal-reasoning",
    "title": "Trust and Causal Reasoning",
    "section": "Causal Reasoning",
    "text": "Causal Reasoning\n\n\nLearn more: Amazon Book of Why"
  },
  {
    "objectID": "slides/trust-causal/index.html#trustworthy-ml-initiative-trustml",
    "href": "slides/trust-causal/index.html#trustworthy-ml-initiative-trustml",
    "title": "Trust and Causal Reasoning",
    "section": "Trustworthy ML Initiative (TrustML)",
    "text": "Trustworthy ML Initiative (TrustML)\n\n\nLearn more: Trustworthy ML Org"
  },
  {
    "objectID": "slides/trust-causal/index.html#gorilla-api-to-ground-tool-use",
    "href": "slides/trust-causal/index.html#gorilla-api-to-ground-tool-use",
    "title": "Trust and Causal Reasoning",
    "section": "Gorilla API to Ground “Tool Use”",
    "text": "Gorilla API to Ground “Tool Use”\n\n\nLearn more: Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "objectID": "slides/trust-causal/index.html#llama2-accessory",
    "href": "slides/trust-causal/index.html#llama2-accessory",
    "title": "Trust and Causal Reasoning",
    "section": "LLaMA2-Accessory",
    "text": "LLaMA2-Accessory\n\n\nLearn more: LLaMA2-Accessory"
  },
  {
    "objectID": "slides/trust-causal/index.html#multilingual-token-analysis",
    "href": "slides/trust-causal/index.html#multilingual-token-analysis",
    "title": "Trust and Causal Reasoning",
    "section": "MultiLingual Token Analysis",
    "text": "MultiLingual Token Analysis\n\n\nLearn more: Video: LLaMA2 Multilingual Models and Fine Tuning\nGoogle Collab Notebook:"
  },
  {
    "objectID": "slides/trust-causal/index.html#metaai-dinov2",
    "href": "slides/trust-causal/index.html#metaai-dinov2",
    "title": "Trust and Causal Reasoning",
    "section": "MetaAI DINOv2",
    "text": "MetaAI DINOv2\n\n\nLearn more: DINOv2 by Meta AI"
  },
  {
    "objectID": "slides/trust-causal/index.html#metaai-dinov2-huggingface-release",
    "href": "slides/trust-causal/index.html#metaai-dinov2-huggingface-release",
    "title": "Trust and Causal Reasoning",
    "section": "MetaAI DINOv2 Huggingface Release",
    "text": "MetaAI DINOv2 Huggingface Release\n\n\nLearn more: DINOv2 by Meta AI Huggingface"
  },
  {
    "objectID": "slides/trust-causal/index.html#example-notebook-for-dinov2-semantic-segmentation",
    "href": "slides/trust-causal/index.html#example-notebook-for-dinov2-semantic-segmentation",
    "title": "Trust and Causal Reasoning",
    "section": "Example notebook for DINOv2 Semantic Segmentation",
    "text": "Example notebook for DINOv2 Semantic Segmentation\n\n\nLearn more: Linear probing of DINOv2 for semantic segmentation"
  },
  {
    "objectID": "slides/trust-causal/index.html#linear-regression-model-for-reasoning",
    "href": "slides/trust-causal/index.html#linear-regression-model-for-reasoning",
    "title": "Trust and Causal Reasoning",
    "section": "Linear Regression Model for Reasoning",
    "text": "Linear Regression Model for Reasoning\n\n\nLearn more: TART: A plug-and-play Transformer module for task-agnostic reasoning"
  },
  {
    "objectID": "slides/trust-causal/index.html#linear-regression-model-for-reasoning-1",
    "href": "slides/trust-causal/index.html#linear-regression-model-for-reasoning-1",
    "title": "Trust and Causal Reasoning",
    "section": "Linear Regression Model for Reasoning",
    "text": "Linear Regression Model for Reasoning\n\n\nLearn more: TART: A plug-and-play Transformer module for task-agnostic reasoning"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About: Laboratory for Assured AI Applications Development (LA3D)",
    "section": "",
    "text": "AI Success Factors: Engineering Trust in Deployments is an initiative of the Laboratory for Assured AI Applications Development (LA3D) at the University of Notre Dame’s Center for Research Computing. This blog delves into the intricate dynamics of artificial intelligence (AI) research and development, with a strong emphasis on AI engineering, trust, and knowledge engineering.\nAs the pace of AI evolution accelerates, understanding the key factors for successful deployment becomes critical. This blog focuses on the intersection of trust and engineering, two pivotal components that determine the efficacy and acceptability of AI applications in various societal contexts.\nThe Laboratory for Assured AI Applications Development operates with a commitment to “translational research,” a methodology that drives us to convert theoretical advancements in AI into practical applications. This commitment extends to the creation of an efficient and secure AI cyberinfrastructure, designed to support the deployment of trustworthy and effective AI applications.\nAI Success Factors serves as a communication medium, helping to disseminate the work conducted by our lab and facilitating knowledge exchange within the AI community. We invite active engagement and encourage thoughtful discussions, aiming to foster a collaborative atmosphere that enriches our collective understanding of AI.\nOur vision for AI Success Factors is to contribute to an AI future marked by trusted and efficient applications, where AI engineering is comprehensible, accessible, and actionable. The blog aims to provide insights and perspectives that enable readers to grasp the nuances of AI deployment, assisting them in navigating the ever-evolving landscape of AI research and development.\nWhether you are a student, a professional, or an enthusiast interested in the progression of AI, AI Success Factors offers a wealth of knowledge and insights into the key aspects that shape the success of AI deployments. Through this platform, we hope to further the understanding and acceptance of AI and its many transformative possibilities.\nCertainly, here is a possible addition to your “About” page that discusses the tools used to create and manage the blog:"
  },
  {
    "objectID": "about.html#about-ai-success-factors-engineering-trust-in-deployments",
    "href": "about.html#about-ai-success-factors-engineering-trust-in-deployments",
    "title": "About: Laboratory for Assured AI Applications Development (LA3D)",
    "section": "",
    "text": "AI Success Factors: Engineering Trust in Deployments is an initiative of the Laboratory for Assured AI Applications Development (LA3D) at the University of Notre Dame’s Center for Research Computing. This blog delves into the intricate dynamics of artificial intelligence (AI) research and development, with a strong emphasis on AI engineering, trust, and knowledge engineering.\nAs the pace of AI evolution accelerates, understanding the key factors for successful deployment becomes critical. This blog focuses on the intersection of trust and engineering, two pivotal components that determine the efficacy and acceptability of AI applications in various societal contexts.\nThe Laboratory for Assured AI Applications Development operates with a commitment to “translational research,” a methodology that drives us to convert theoretical advancements in AI into practical applications. This commitment extends to the creation of an efficient and secure AI cyberinfrastructure, designed to support the deployment of trustworthy and effective AI applications.\nAI Success Factors serves as a communication medium, helping to disseminate the work conducted by our lab and facilitating knowledge exchange within the AI community. We invite active engagement and encourage thoughtful discussions, aiming to foster a collaborative atmosphere that enriches our collective understanding of AI.\nOur vision for AI Success Factors is to contribute to an AI future marked by trusted and efficient applications, where AI engineering is comprehensible, accessible, and actionable. The blog aims to provide insights and perspectives that enable readers to grasp the nuances of AI deployment, assisting them in navigating the ever-evolving landscape of AI research and development.\nWhether you are a student, a professional, or an enthusiast interested in the progression of AI, AI Success Factors offers a wealth of knowledge and insights into the key aspects that shape the success of AI deployments. Through this platform, we hope to further the understanding and acceptance of AI and its many transformative possibilities.\nCertainly, here is a possible addition to your “About” page that discusses the tools used to create and manage the blog:"
  },
  {
    "objectID": "about.html#our-toolset",
    "href": "about.html#our-toolset",
    "title": "About: Laboratory for Assured AI Applications Development (LA3D)",
    "section": "Our Toolset",
    "text": "Our Toolset\nAI Success Factors: Engineering Trust in Deployments is more than just a blog - it’s a demonstration of how modern data science tools can streamline and simplify complex processes.\nOur blogging platform is built using Quarto, an open-source scientific and technical publishing system known for its flexibility and robustness. Quarto has the unique ability to support a wide range of content types, including Jupyter notebooks, R Markdown, and even Python or R scripts. Quarto’s distinctive feature is its ability to include executable code within the documents from various languages such as Python, R, and others. This allows us to create data-driven blog posts where the output - be it a graph, a table, or other data representations - is generated directly from the included code. This adds an interactive element to our posts, giving readers a deeper understanding of the concepts and analyses presented. Quarto supports multiple output formats including HTML, PDF, EPUB, and Word, providing us the versatility to tailor our content to suit different needs. This flexible tool, coupled with its capability to handle larger projects made up of multiple files, allows us to efficiently manage our blog while maintaining consistency and quality across our posts. Quarto presentations are also used to generate revealjs presentations for weekly slide shows. Quarto forms the basis for Fast.ai’s nbdev framework for exploritory programming allowing for software development in Jupyter Notebooks and automated generation of Python modules and documentation from the notebooks.\nDevelopment Containers serve as our primary environment for data science, providing a consistent and replicable framework for running our analyses. These containerized environments allow us to standardize our work and ensure that our research is reproducible and reliable. The dev container uses the Mamba solver to provision the python environment from the specification in the environment.yml.\nWe use Visual Studio Code (VS Code) as our primary code editor, taking advantage of its rich ecosystem of extensions and in-built features. VS Code’s Jupyter notebook support allows us to interactively develop and visualize our data models directly within the editor, enabling us to produce intuitive, data-driven narratives for our blog posts.\nAll of our content is version controlled and hosted on GitHub. GitHub’s robust versioning system allows us to effectively manage changes, track progress, and ensure that every piece of content we publish is up to date and accurate.\nThe blog is published using GitHub Page, a platform that simplifies the deployment process and seamlessly integrates with our existing GitHub repository. This setup enables us to provide a reliable, accessible resource for our readers, irrespective of where they are or when they choose to access our content.\nFinally, we utilize GitHub Code Spaces to create a fully-featured, cloud-hosted development environment that can be accessed from any device. This not only allows us to work from anywhere but also ensures that our setup can be easily replicated by other researchers and developers who wish to explore our code.\nEach tool in our stack has been chosen for its ability to facilitate efficient, reliable, and transparent data science. By sharing our toolset, we hope to provide insight into our workflows and encourage a culture of open, reproducible research within the AI community."
  },
  {
    "objectID": "about.html#generative-ai-in-content-creation",
    "href": "about.html#generative-ai-in-content-creation",
    "title": "About: Laboratory for Assured AI Applications Development (LA3D)",
    "section": "Generative AI in Content Creation",
    "text": "Generative AI in Content Creation\nIn creating AI Success Factors: Engineering Trust in Deployments, we harness the power of artificial intelligence in conjunction with human expertise. We utilize advanced Generative AI models such as ChatGPT, Bing Chat, and Anthropic Claude in a guided, iterative process that combines the best of AI and human capabilities.\nOur use of Generative AI begins with the drafting stage. These models, trained on extensive datasets, generate human-like text that matches our specified tone and style, providing us with a solid foundation for each blog post. This allows us to focus on the larger narrative without getting mired in the nitty-gritty details from the outset.\nThe AI-generated drafts, while sophisticated, are just the starting point. We employ a methodology that involves guiding the AI to improve its output. This includes providing more detailed prompts, specifying the format we want the output in, or asking the model to think step-by-step before settling on a conclusion.\nOnce we have a draft that we’re satisfied with, it’s time for human intervention. Our team reviews the AI-generated content, refining and editing it to ensure it maintains the standards of accuracy, relevance, and depth that our readers expect. This process leverages critical human skills of creativity, critical thinking, and domain expertise that even the most advanced AI can’t replicate.\nThis approach exemplifies our vision of a symbiotic relationship between AI and humans. By effectively integrating Generative AI into our content creation process, we not only boost our productivity and efficiency but also demonstrate the practical application of AI technologies in real-world scenarios. Thus, the blog becomes a testament to our commitment to translational research and our pursuit of an AI-integrated future."
  }
]