---
title: "A Summer of Nuggets"
author: "Charles F Vardeman II"
date: "2023-08-28"
embed-resources: true
format: 
    revealjs:
        slide-number: true
categories: [nuggets,trustedAI,LLMs]
---

## And a Reminder of Some Before...{.center}

## Where we started...

:::{.r-stack}
![](Gdrive.png){.center width="800" height="576"}
:::


## Where we're going...

:::{.r-stack}
![](la3d_nuggets.png){.center width="1000" height="494"}
:::

::: footer
[AI Success Factors: Engineering Trust in Deployments](https://la3d.github.io/nuggets/slideindex.html)
:::

## Dr. Vardeman, this is a lot of material!{.center}

## This is meant to provide something of a "Roadmap"!{.center}

## Old School AI and the Web...

::: columns
::: {.column width="40%"}
A vision of "Ontologies", "Linked Data", and "Software Agents"...
:::

::: {.column width="60%"}
![](the_semantic_web.png){.center}
:::
:::

::: footer
[Tim Berners-Lee, James Hendler, and Ora Lassila. "The Semantic Web." Scientific American 284, no. 5 (2001): 34--43. https://lassila.org/publications/2001/SciAm.html](https://lassila.org/publications/2001/SciAm.html)
:::


## Old School "Cool" AI and the Web (2001)...

::: columns
::: {.column width="40%"}
The semantic web had a vision of **Agents** with **Shared Understanding through Ontologies**, the ability to **Use Tools Like the Web** and Consume a Web of **Linked Data as Distibuted Knowledge Graphs**.
:::

::: {.column width="60%"}
![](vision_of_agents.png){.center width="399" height="500"}
:::
:::

::: footer
[Tim Berners-Lee, James Hendler, and Ora Lassila. "The Semantic Web." Scientific American 284, no. 5 (2001): 34--43. https://lassila.org/publications/2001/SciAm.html](https://lassila.org/publications/2001/SciAm.html)
:::

## What's an Ontology?

![](ontology.png)

::: footer
[Tom Gruber. “Ontology.” In Encyclopedia of Database Systems, edited by Ling Liu and M. Tamer Özsu, 1–3. New York, NY: Springer, 2016. https://doi.org/10.1007/978-1-4899-7993-3_1318-2.](https://doi.org/10.1007/978-1-4899-7993-3_1318-2)
:::

## Ontology Design Patterns


::: columns
::: {.column width="40%"}
![](odp_definition.png)
:::

::: {.column width="60%"}
![](odp_example.png){.center}
:::
:::

::: footer
[Eva Blomqvist, Pascal Hitzler, Krzysztof Janowicz, Adila Krisnadhi, Tom Narock, and Monika Solanki. “Considerations Regarding Ontology Design Patterns.” Semantic Web 7, no. 1 (November 10, 2015): 1–7. https://doi.org/10.3233/SW-150202.](https://people.cs.ksu.edu/~hitzler/pub2/swj-7-1-2015.pdf)
:::

## Google Search and the "Semantic Web"

::: columns
::: {.column width="50%"}
![](google_kg.png)
:::

::: {.column width="50%"}
![](google_kg2.png){.center}
:::
:::

:::footer
[Google. “Introducing the Knowledge Graph: Things, Not Strings,” May 16, 2012. https://blog.google/products/search/introducing-knowledge-graph-things-not/.](https://blog.google/products/search/introducing-knowledge-graph-things-not/)

[Google. “A Reintroduction to Our Knowledge Graph and Knowledge Panels,” May 20, 2020. https://blog.google/products/search/about-knowledge-graph-and-knowledge-panels/.](https://blog.google/products/search/about-knowledge-graph-and-knowledge-panels/)
:::


## Knowledge Graphs

::: columns
::: {.column width="80%"}
![](kg_example.png)
:::

::: {.column width="20%"}
![](kgbook.png){.center}
:::
:::

::: footer
[Hogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer, 2021. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.](https://kgbook.org/)
:::

## Knowledge Graphs

::: columns
::: {.column width="20%"}
What's a Knowledge Graph?
:::

::: {.column width="80%"}
![](kg_definition.png){.center}
:::
:::

::: footer
[Hogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer, 2021. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.](https://kgbook.org/)
:::

## AI in 2023..{.center}

## The "AI Social Disruption"

::: {.r-stack}
![](ai_disruption.png){.fragment width="450" height="600"}
:::

::: footer
[James Gary, "AI -- The Social Disruption", AI Magazine 42, no. 1 (April 12, 2021), Cover.](https://ojs.aaai.org/aimagazine/index.php/aimagazine/issue/view/267)
:::

## Pretrained Foundation Models...

:::{.r-stack}
![](foundation_models.png){.center width="1000" height="340"}
:::

:::footer
[Zhou, Ce, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, et al. “A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.” arXiv, May 1, 2023. https://doi.org/10.48550/arXiv.2302.09419.](https://doi.org/10.48550/arXiv.2302.09419)
:::

## Evolution of Large Language Models

:::{.r-stack}
![](evolution_llms.png){.center width="600" height="593"}
:::

::: footer
Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. “Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.” arXiv, April 27, 2023. https://doi.org/10.48550/arXiv.2304.13712.
:::

## How to Build a ChatGPT? {.center}

## "The state of GPT" -- You should watch this!

:::{.r-stack}
![](state_of_gpt.png){.center width="800" height="510"}
:::

::: footer
[Andrej Karpathy, "State of GPT" | BRK216HFS, Microsoft Build, 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

## "The state of GPT"

:::{.r-stack}
![](state_of_gpt_pipeline.png){.center width="900" height="510"}
:::

::: footer
[Andrej Karpathy, "State of GPT" | BRK216HFS, Microsoft Build, 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

## Large Language Models from a more general view...

:::{.r-stack}
![](willison_wordcamp.png){.center width="421" height="500"}
:::

::: footer
[“Making Large Language Models Work for You.” Accessed August 27, 2023. https://simonwillison.net/2023/Aug/27/wordcamp-llms/.](https://simonwillison.net/2023/Aug/27/wordcamp-llms/)
:::

## Text to numbers...

:::{.r-stack}
![](tiktokenizer.png){.center width="900" height="548"}
:::

::: footer
[“Tiktokenizer.” Accessed August 27, 2023. https://tiktokenizer.vercel.app/.](https://tiktokenizer.vercel.app/)
:::

## Base models create general representations through "Pre-Training" (GPT)

:::{.r-stack}
![](gpt-abstract.png){.center width="434" height="400"}
:::

::: footer
[Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. “Improving Language Understanding by Generative Pre-Training,” n.d.](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
:::

## Base models create general representations through "Pre-Training" (GPT)

:::{.r-stack}
![](gpt-arch.png){.center width="900" height="479"}
:::

::: footer
[Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. “Improving Language Understanding by Generative Pre-Training,” n.d.](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
:::

## "Pre-Training Datasets?"{.center}

## LLama: Open and Efficient Foundation Language Models


::: columns
::: {.column width="50%"}
![](LLama.png)
:::

::: {.column width="50%"}
![](LLama_data.png)
:::
:::

::: footer
[Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. “LLaMA: Open and Efficient Foundation Language Models.” arXiv, February 27, 2023. https://doi.org/10.48550/arXiv.2302.13971.](https://doi.org/10.48550/arXiv.2302.13971)
:::

## LLama: Open and Efficient Foundation Language Models


::: columns
::: {.column width="50%"}
![](LLama-books.png)
:::

::: {.column width="50%"}
![](LLama_data.png)
:::
:::

::: footer
[Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. “LLaMA: Open and Efficient Foundation Language Models.” arXiv, February 27, 2023. https://doi.org/10.48550/arXiv.2302.13971.](https://doi.org/10.48550/arXiv.2302.13971)
:::

## Gao et al. "The Pile"?

::: columns
::: {.column width="50%"}
![](the_pile.png)
:::

::: {.column width="50%"}
![](pile-books3.png)
![](pile-books3-2.png)
:::
:::

::: footer
[Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling.” arXiv, December 31, 2020. https://doi.org/10.48550/arXiv.2101.00027.](https://doi.org/10.48550/arXiv.2101.00027)
:::

## Gao et al. "The Pile"?

::: columns
::: {.column width="50%"}
![](presser-2020.png)
:::

::: {.column width="50%"}
![](pile-books3.png)
![](pile-books3-2.png)
:::
:::

::: footer
[Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling.” arXiv, December 31, 2020. https://doi.org/10.48550/arXiv.2101.00027.](https://doi.org/10.48550/arXiv.2101.00027)
:::

## What could go wrong?{.center}

## What could go wrong?

:::{.r-stack}
![](cnn-silverman.png){.center width="500" height="533"}
:::

::: footer
[“Sarah Silverman Sues OpenAI and Meta Alleging Copyright Infringement | CNN Business.” Accessed August 27, 2023. https://www.cnn.com/2023/07/10/tech/sarah-silverman-openai-meta-lawsuit/index.html.](https://www.cnn.com/2023/07/10/tech/sarah-silverman-openai-meta-lawsuit/index.html)
:::

## What could go wrong?

::: columns
::: {.column width="50%"}
![](atlantic-books3.png)
:::

::: {.column width="50%"}
"Would I forbid the teaching (if that is the word) of my stories to computers? Not even if I could. I might as well be King Canute, forbidding the tide to come in. Or a Luddite trying to stop industrial progress by hammering a steam loom to pieces." -- Stephen King
:::
:::

::: footer
[Reisner, Alex. “Revealed: The Authors Whose Pirated Books Are Powering Generative AI.” The Atlantic, August 19, 2023. https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/.](https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/)

[King, Stephen. “Stephen King: My Books Were Used to Train AI.” The Atlantic, August 23, 2023. https://www.theatlantic.com/books/archive/2023/08/stephen-king-books-ai-writing/675088/.](https://www.theatlantic.com/books/archive/2023/08/stephen-king-books-ai-writing/675088/)
:::

## (GPT-3) Language Models are Few-Shot Learners (2020)

:::{.r-stack}
![](gpt-3-few-shot.png){.center width="512" height="590"}
:::

::: footer
[Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” arXiv, July 22, 2020. https://doi.org/10.48550/arXiv.2005.14165.](https://doi.org/10.48550/arXiv.2005.14165)
:::


## (GPT-3) In-Context Learning

:::{.r-stack}
![](gpt3-incontext.png){.center width="850" height="512"}
:::

::: footer
[Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” arXiv, July 22, 2020. https://doi.org/10.48550/arXiv.2005.14165.](https://doi.org/10.48550/arXiv.2005.14165)
:::

## (GPT-3) Instruct-GPT Reinforcement Learning from Human Feedback

:::{.r-stack}
![](instruct-gpt.png){.center width="497" height="500"}
:::

::: footer
[Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. “Training Language Models to Follow Instructions with Human Feedback.” arXiv, March 4, 2022. https://doi.org/10.48550/arXiv.2203.02155.](https://doi.org/10.48550/arXiv.2203.02155.)
:::

## (GPT-3) Instruct-GPT Reinforcement Learning from Human Feedback

:::{.r-stack}
![](instruct-gpt-method.png){.center width="751" height="550"}
:::

::: footer
[Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. “Training Language Models to Follow Instructions with Human Feedback.” arXiv, March 4, 2022. https://doi.org/10.48550/arXiv.2203.02155.](https://doi.org/10.48550/arXiv.2203.02155.)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](sparks-agi.png){.center width="744" height="550"}
:::

::: footer
[Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, et al. “Sparks of Artificial General Intelligence: Early Experiments with GPT-4.” arXiv, April 13, 2023. https://doi.org/10.48550/arXiv.2303.12712.](https://doi.org/10.48550/arXiv.2303.12712)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](sparks_unicorn.png){.center width="1065" height="550"}
:::

::: footer
[Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, et al. “Sparks of Artificial General Intelligence: Early Experiments with GPT-4.” arXiv, April 13, 2023. https://doi.org/10.48550/arXiv.2303.12712.](https://doi.org/10.48550/arXiv.2303.12712)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](sparks_unicorn_time.png){.center width="1000" height="338"}
:::

::: footer
[Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, et al. “Sparks of Artificial General Intelligence: Early Experiments with GPT-4.” arXiv, April 13, 2023. https://doi.org/10.48550/arXiv.2303.12712.](https://doi.org/10.48550/arXiv.2303.12712)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](youtube-sparks.png){.center width="723" height="550"}
:::

::: footer
[Sparks of AGI: Early Experiments with GPT-4, 2023. https://www.youtube.com/watch?v=qbIk7-JPB2c.](https://www.youtube.com/watch?v=qbIk7-JPB2c)
:::

## LLM Behavior Changes with Time!

:::{.r-stack}
![](../tai_testing/gpt_behavior_drift_chart.png){.center width="591" height="650"}
:::

::: footer
[Chen, Lingjiao, Matei Zaharia, and James Zou. “How Is ChatGPT’s Behavior Changing over Time?” arXiv, July 18, 2023. http://arxiv.org/abs/2307.09009.](http://arxiv.org/abs/2307.09009)
:::

## (GPT-4) OpenAI Reinforcement Learning -- "Towards TruthGPT"

:::{.r-stack}
![](youtube-truthgpt.png){.center width="812" height="600"}
:::

::: footer
[John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges, 2023. https://www.youtube.com/watch?v=hhiLw5Q_UFg.](https://www.youtube.com/watch?v=hhiLw5Q_UFg.)
:::

## (GPT-4) OpenAI Reinforcement Learning -- "Conceptual Models"

:::{.r-stack}
![](youtube-truthgpt-kg.png){.center width="828" height="600"}
:::

::: footer
[John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges, 2023. https://www.youtube.com/watch?v=hhiLw5Q_UFg.](https://www.youtube.com/watch?v=hhiLw5Q_UFg.)
:::


## (Claude) "Constitutional AI"

:::{.r-stack}
![](constitutional_ai.png){.center width="585" height="650"}
:::

::: footer
[Anthropic. “Claude’s Constitution.” Accessed August 28, 2023. https://www.anthropic.com/index/claudes-constitution.](https://www.anthropic.com/index/claudes-constitution)

[Bai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, et al. “Constitutional AI: Harmlessness from AI Feedback.” arXiv, December 15, 2022. https://doi.org/10.48550/arXiv.2212.08073.](https://doi.org/10.48550/arXiv.2212.08073)
:::

## (Claude) "Foundation Model in AWS Bedrock"

::: columns
::: {.column width="40%"}
![](claude-2.png)
:::

::: {.column width="60%"}
![](claude-2-bedrock.png){.center width="405" height="550"}
:::
:::


::: footer
[Anthropic. “Claude 2.” Accessed August 28, 2023. https://www.anthropic.com/index/claude-2.](https://www.anthropic.com/index/claude-2)

[Anthropic. “Claude 2 on Amazon Bedrock.” Accessed August 28, 2023. https://www.anthropic.com/index/claude-2-amazon-bedrock.
](https://www.anthropic.com/index/claude-2-amazon-bedrock)
:::

## (Llama 2)"Open-License" Large Language Models

::: {.r-stack}
![](llama2.png){.center width="617" height="650" }
:::

::: footer
[Meta AI. “Meta and Microsoft Introduce the Next Generation of Llama.” Accessed August 28, 2023. https://ai.meta.com/blog/llama-2/.
](https://ai.meta.com/blog/llama-2/)

[Meta AI. “Llama 2.” Accessed August 28, 2023. https://ai.meta.com/llama-project.
](https://ai.meta.com/llama-project)
:::

## "The state of GPT" Recommendations

:::{.r-stack}
![](karpathy-guidance.png){.center width="1076" height="600"}
:::

::: footer
[Andrej Karpathy, "State of GPT" | BRK216HFS, Microsoft Build, 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::


## Reasoning...{.center}

## (GPT-3) Large Language Models are Zero Shot Reasoners (Chain-of-Thought Reasoning)

:::{.r-stack}
![](CoT-Reasoning.png){.center width="520" height="500"}
:::

::: footer
[Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. “Large Language Models Are Zero-Shot Reasoners.” arXiv, January 29, 2023. https://doi.org/10.48550/arXiv.2205.11916.](https://doi.org/10.48550/arXiv.2205.11916)
:::

## (GPT-3) Large Language Models are Zero Shot Reasoners (Chain-of-Thought Reasoning)

:::{.r-stack}
![](CoT-Reasoning-example.png){.center width="694" height=500"}
:::

::: footer
[Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. “Large Language Models Are Zero-Shot Reasoners.” arXiv, January 29, 2023. https://doi.org/10.48550/arXiv.2205.11916.](https://doi.org/10.48550/arXiv.2205.11916)
:::

## ReAct: Synergizing Reasoning and Acting in Language Models

:::{.r-stack}
![](ReAcT.png){.center width="639" height=600"}
:::

::: footer
[“ReAct: Synergizing Reasoning and Acting in Language Models.” Accessed August 28, 2023. https://react-lm.github.io/.](https://react-lm.github.io/.])

[Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629.](http://arxiv.org/abs/2210.03629)
:::

## ReAct: Synergizing Reasoning and Acting in Language Models

:::{.r-stack}
![](react-diagram.png){.center width="1000" height=270"}
:::

::: footer
[“ReAct: Synergizing Reasoning and Acting in Language Models.” Accessed August 28, 2023. https://react-lm.github.io/.](https://react-lm.github.io/.])

[Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629.](http://arxiv.org/abs/2210.03629)
:::

## ReAct: Synergizing Reasoning and Acting in Language Models

:::{.r-stack}
![](react-prompt.png){.center width="611" height=600"}
:::

::: footer
[“ReAct: Synergizing Reasoning and Acting in Language Models.” Accessed August 28, 2023. https://react-lm.github.io/.](https://react-lm.github.io/.])

[Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629.](http://arxiv.org/abs/2210.03629)
:::

## Prompt Engineering

:::{.r-stack}
![](prompt-engineering.png){.center width="625" height=600"}
:::

::: footer
“Prompt Engineering Guide.” Accessed August 22, 2023. https://www.promptingguide.ai/.
:::

## Large Language Models are Semantic Reasoners

:::{.r-stack}
![](semantic-reasoners.png){.center width="701" height=600"}
:::

::: footer
[Tang, Xiaojuan, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. “Large Language Models Are In-Context Semantic Reasoners Rather than Symbolic Reasoners.” arXiv, June 8, 2023. http://arxiv.org/abs/2305.14825.]()
:::

## Large Language Models are Semantic Reasoners


::: columns
::: {.column width="40%"}
![](semantic-reasoners-exp.png)
:::

::: {.column width="60%"}
![](semantic-reasoners-types.png)
:::
:::

::: footer
[Tang, Xiaojuan, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. “Large Language Models Are In-Context Semantic Reasoners Rather than Symbolic Reasoners.” arXiv, June 8, 2023. http://arxiv.org/abs/2305.14825.]()
:::

## Our Research...{.center}

## Retrevial Augmented Generation

:::{.r-stack}
![](rag-claude.png){.center width="900" height=683"}
:::

## Knowledge Graphs and LLMs -- Must Read!

:::{.r-stack}
![](../tai-tools/PLM_vs_KG.png){.center width="927" height="600"}
:::

::: footer
[Pan, Shirui, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. “Unifying Large Language Models and Knowledge Graphs: A Roadmap.” arXiv, June 14, 2023. http://arxiv.org/abs/2306.08302.](http://arxiv.org/abs/2306.08302)
:::

## Knowledge Graphs and Frameworks

:::{.r-stack}
![](kg-llm-frameworks.png){.center width="616" height="650"}
:::

::: footer
[Pan, Shirui, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. “Unifying Large Language Models and Knowledge Graphs: A Roadmap.” arXiv, June 14, 2023. http://arxiv.org/abs/2306.08302.](http://arxiv.org/abs/2306.08302)
:::



## What About Trusted AI?{.center}

## How to talk about LLMs? Must Read!

:::{.r-stack}
![](talking_about_llms.png){.center width="665" height="600"}
:::

::: footer
[Shanahan, Murray. “Talking About Large Language Models.” arXiv, February 16, 2023. https://doi.org/10.48550/arXiv.2212.03551.](https://doi.org/10.48550/arXiv.2212.03551)
:::
