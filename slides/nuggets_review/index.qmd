---
title: "A Summer of Nuggets"
author: "Charles F Vardeman II"
date: "2023-08-28"
embed-resources: true
format: 
    revealjs:
        slide-number: true
categories: [nuggets,trustedAI,LLMs]
---


## Where we started...

:::{.r-stack}
![](Gdrive.png){.center width="800" height="576"}
:::


## Where we're going...

:::{.r-stack}
![](la3d_nuggets.png){.center width="1000" height="494"}
:::

::: footer
[AI Success Factors: Engineering Trust in Deployments](https://la3d.github.io/nuggets/slideindex.html)
:::


## This is meant to provide something of a "Roadmap"!{.center}

## [Video Link](http://tiny.cc/6uhavz){.center}

## "Old School AI" and the Web...

::: columns
::: {.column width="40%"}
A vision of "Ontologies", "Linked Data", and "Software Agents"...
:::

::: {.column width="60%"}
![](the_semantic_web.png){.center}
:::
:::

::: footer
[Tim Berners-Lee, James Hendler, and Ora Lassila. "The Semantic Web." Scientific American 284, no. 5 (2001): 34--43. https://lassila.org/publications/2001/SciAm.html](https://lassila.org/publications/2001/SciAm.html)
:::


## Old School "Cool" AI and the Web (2001)...

::: columns
::: {.column width="40%"}
The semantic web had a vision of **Agents** with **Shared Understanding through Ontologies**, the ability to **Use Tools Like the Web** and Consume a Web of **Linked Data as Distibuted Knowledge Graphs**.
:::

::: {.column width="60%"}
![](vision_of_agents.png){.center width="399" height="500"}
:::
:::

::: footer
[Tim Berners-Lee, James Hendler, and Ora Lassila. "The Semantic Web." Scientific American 284, no. 5 (2001): 34--43. https://lassila.org/publications/2001/SciAm.html](https://lassila.org/publications/2001/SciAm.html)
:::

## What's an Ontology?

![](ontology.png)

::: footer
[Tom Gruber. “Ontology.” In Encyclopedia of Database Systems, edited by Ling Liu and M. Tamer Özsu, 1–3. New York, NY: Springer, 2016. https://doi.org/10.1007/978-1-4899-7993-3_1318-2.](https://doi.org/10.1007/978-1-4899-7993-3_1318-2)
:::

## Ontology Design Patterns


::: columns
::: {.column width="40%"}
![](odp_definition.png)
:::

::: {.column width="60%"}
![](odp_example.png){.center}
:::
:::

::: footer
[Eva Blomqvist, Pascal Hitzler, Krzysztof Janowicz, Adila Krisnadhi, Tom Narock, and Monika Solanki. “Considerations Regarding Ontology Design Patterns.” Semantic Web 7, no. 1 (November 10, 2015): 1–7. https://doi.org/10.3233/SW-150202.](https://people.cs.ksu.edu/~hitzler/pub2/swj-7-1-2015.pdf)
:::

## Google Search and the "Semantic Web"

::: columns
::: {.column width="50%"}
![](google_kg.png)
:::

::: {.column width="50%"}
![](google_kg2.png){.center}
:::
:::

:::footer
[Google. “Introducing the Knowledge Graph: Things, Not Strings,” May 16, 2012. https://blog.google/products/search/introducing-knowledge-graph-things-not/.](https://blog.google/products/search/introducing-knowledge-graph-things-not/)

[Google. “A Reintroduction to Our Knowledge Graph and Knowledge Panels,” May 20, 2020. https://blog.google/products/search/about-knowledge-graph-and-knowledge-panels/.](https://blog.google/products/search/about-knowledge-graph-and-knowledge-panels/)
:::


## Knowledge Graphs

::: columns
::: {.column width="80%"}
![](kg_example.png)
:::

::: {.column width="20%"}
![](kgbook.png){.center}
:::
:::

::: footer
[Hogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer, 2021. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.](https://kgbook.org/)
:::

## Knowledge Graphs

::: columns
::: {.column width="20%"}
What's a Knowledge Graph?
:::

::: {.column width="80%"}
![](kg_definition.png){.center}
:::
:::

::: footer
[Hogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer, 2021. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.](https://kgbook.org/)
:::

## AI in 2023..{.center}

## The "AI Social Disruption"

::: {.r-stack}
![](ai_disruption.png){.fragment width="450" height="600"}
:::

::: footer
[James Gary, "AI -- The Social Disruption", AI Magazine 42, no. 1 (April 12, 2021), Cover.](https://ojs.aaai.org/aimagazine/index.php/aimagazine/issue/view/267)
:::

## Pretrained Foundation Models...

:::{.r-stack}
![](foundation_models.png){.center width="1000" height="340"}
:::

:::footer
[Zhou, Ce, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, et al. “A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.” arXiv, May 1, 2023. https://doi.org/10.48550/arXiv.2302.09419.](https://doi.org/10.48550/arXiv.2302.09419)
:::

## Evolution of Large Language Models

:::{.r-stack}
![](evolution_llms.png){.center width="600" height="593"}
:::

::: footer
[Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. “Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.” arXiv, April 27, 2023. https://doi.org/10.48550/arXiv.2304.13712.](https://doi.org/10.48550/arXiv.2304.13712)
:::

## Transformer:"Attention is all you need"

:::{.r-stack}
![](transformer-arch.png){.center width="449" height="650"}
:::

::: footer
[Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762.](https://doi.org/10.48550/arXiv.1706.03762)
:::

## *"Explain the transformer neural network architecture to a College Junior Computer Science student. Use the style of a good science communicator like Richard Feynman."* {.center}

## Transformers explained by ChatGPT{.smaller}
Alright, let's get into it! Imagine you're at a party and you're trying to catch snippets of multiple conversations. You're not just listening to the words, but also noticing who's talking to whom, the tone, the context, and you're making judgments about what’s important or not. That’s kinda what the Transformer architecture does, but for sequences of data like sentences or time-series data.

At the core, a Transformer has two main parts: the Encoder and the Decoder. Each has multiple identical layers stacked on top of each other.

1. **Encoder**: Takes in the sequence (like a sentence "Hello, how are you?") and processes it as a whole. It outputs a sort of "summary" or "context" for each word in the sequence.
   - **Self-Attention**: Each word checks out all the other words and figures out which ones are important to it. The magic formula called attention scores helps in this.
   - **Feed-forward neural networks**: After attention, each word passes through a mini neural network that's the same for all words, transforming it yet again.

## Transformers explained by ChatGPT{.smaller}
2. **Decoder**: Takes these encodings and starts generating the output sequence (say, a translation of the original sentence into another language).
   - **Masked Self-Attention**: Similar to the encoder, but in a way that it can't cheat and look at future words in the output sequence.
   - **Encoder-Decoder Attention**: The decoder takes a peek at the encoder’s output to better understand the context and produce the correct next word.
   - **Feed-forward neural networks**: Same drill as the encoder, another mini network for transforming words.

## Transformers explained by ChatGPT
So the magic here is mainly in how words interact with each other through the attention mechanism. Each word looks at all the other words to decide how much focus it should get, like how you tune in to different voices at a party. It does this not once, but multiple times, diving deeper and deeper into the context each time.

It’s like a super-efficient assembly line for sequences, enabling models like me to generate text or translate languages at scale. And this architecture has been the basis for a lot of breakthroughs in natural language processing. It's versatile, parallelizable, and mighty powerful!


## *"Some LLM Neural Networks use encoder-decoder networks. Some LLM neural networks are encoder only and some are decoder only like GPT. Explain that to a College Junior Computer Science student. Use the style of a good science communicator like Richard Feynman."* {.center}

## Transformers explained by ChatGPT {.smaller}
Ah, good question! Think of it like this: you've got three types of orchestras. One plays both classical and jazz seamlessly (encoder-decoder), another specializes only in classical (encoder), and the last one is all about that jazz (decoder).

1. **Encoder-Decoder Networks**: These are like your all-rounder orchestras. In the machine learning world, these models are often used for tasks like machine translation. The encoder part processes the input language, let's say, English, and the decoder part spits out the output in another language, maybe Spanish. They work together like a well-conducted orchestra, each contributing to the final masterpiece.

2. **Encoder-Only Networks**: Imagine an orchestra that's focused entirely on classical music, where each musician feeds into the other to create a beautiful symphony. In tech, these models are good for tasks like text classification or sentiment analysis. The encoder processes the text and summarizes it in a way that lets you say, "Ah, this review is positive!"

## Transformers explained by ChatGPT{.smaller}

3. **Decoder-Only Networks**: Now, these are your jazz specialists, all about improvisation and solos. GPT models, like the one you're talking to, belong to this category. The decoder starts with a seed (your prompt) and keeps generating one word at a time, predicting the next best word until it forms a coherent reply. It doesn't need to look back at an original input sentence like in translation; it just jams out text based on what it has learned during its training.

So, you've got different setups for different gigs. If you need a two-way conversation between languages, you go for the encoder-decoder. If you want to understand the essence of a piece of text, you'd lean on the encoder. And if you want to generate text, like writing or even code, the decoder is your go-to musician. Each has its own style, its own instruments, and its own unique approach to making beautiful "music"—or in our case, useful computations.

## *"How do these relate to the Transformer architecture"* {.center}

## Transformers explained by ChatGPT {.smaller}
Great follow-up! Alright, think of the Transformer architecture as a fancy new instrument—a versatile electric guitar, let's say, that can produce a whole range of sounds from classical to rock to jazz. Now, just like you can use that electric guitar in different types of bands, you can use parts of the Transformer architecture in different types of models.

1. **Encoder-Decoder Networks with Transformers**: When people initially talked about Transformers, they were often referring to the full shebang—both encoder and decoder. In a language translation model, the Transformer's encoder reads an English sentence and compresses its essence. The decoder then takes this essence and generates a Spanish sentence. Both sides are using self-attention, layer normalization, and all those Transformer goodies to do their jobs.

2. **Encoder-Only Transformers**: In some tasks, you don't need the full band; a guitar solo will do. For instance, BERT (Bidirectional Encoder Representations from Transformers) uses just the encoder part of the Transformer architecture. It processes a sentence and spits out a rich, contextual representation of each word, which can then be used for tasks like text classification or filling in blanks.

3. **Decoder-Only Transformers**: Now, what if we only want to jam and improvise? That’s where models like GPT (Generative Pre-trained Transformer) come in. These models use only the decoder part of the Transformer architecture to generate new text. They still use self-attention and feed-forward layers, but they are more about predicting the next note—or in our case, the next word—in a sequence.

## Transformers explained by ChatGPT {.smaller}
So there you go! Just like our versatile electric guitar can be part of a full orchestra, a rock band, or a solo act, different pieces of the Transformer architecture can be used to construct different kinds of neural network models. Each specialized version takes the core principles of the Transformer and applies them in a way that's tailored to specific tasks.

## How to Build a ChatGPT? {.center}

## "The state of GPT" -- You should watch this!

:::{.r-stack}
![](state_of_gpt.png){.center width="800" height="510"}
:::

::: footer
[Andrej Karpathy, "State of GPT" | BRK216HFS, Microsoft Build, 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

## "The state of GPT"

:::{.r-stack}
![](state_of_gpt_pipeline.png){.center width="900" height="510"}
:::

::: footer
[Andrej Karpathy, "State of GPT" | BRK216HFS, Microsoft Build, 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

## Large Language Models from a more general view...

:::{.r-stack}
![](willison_wordcamp.png){.center width="421" height="500"}
:::

::: footer
[“Making Large Language Models Work for You.” Accessed August 27, 2023. https://simonwillison.net/2023/Aug/27/wordcamp-llms/.](https://simonwillison.net/2023/Aug/27/wordcamp-llms/)
:::

## Text to numbers...

:::{.r-stack}
![](tiktokenizer.png){.center width="900" height="548"}
:::

::: footer
[“Tiktokenizer.” Accessed August 27, 2023. https://tiktokenizer.vercel.app/.](https://tiktokenizer.vercel.app/)
:::

## Base models create general representations through "Pre-Training" (GPT)

:::{.r-stack}
![](gpt-abstract.png){.center width="434" height="400"}
:::

::: footer
[Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. “Improving Language Understanding by Generative Pre-Training,” n.d.](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
:::

## Base models create general representations through "Pre-Training" (GPT)

:::{.r-stack}
![](gpt-arch.png){.center width="900" height="479"}
:::

::: footer
[Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. “Improving Language Understanding by Generative Pre-Training,” n.d.](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
:::

## "Pre-Training Datasets?"{.center}

## LLama: Open and Efficient Foundation Language Models


::: columns
::: {.column width="50%"}
![](LLama.png)
:::

::: {.column width="50%"}
![](LLama_data.png)
:::
:::

::: footer
[Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. “LLaMA: Open and Efficient Foundation Language Models.” arXiv, February 27, 2023. https://doi.org/10.48550/arXiv.2302.13971.](https://doi.org/10.48550/arXiv.2302.13971)
:::

## LLama: Open and Efficient Foundation Language Models


::: columns
::: {.column width="50%"}
![](LLama-books.png)
:::

::: {.column width="50%"}
![](LLama_data.png)
:::
:::

::: footer
[Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. “LLaMA: Open and Efficient Foundation Language Models.” arXiv, February 27, 2023. https://doi.org/10.48550/arXiv.2302.13971.](https://doi.org/10.48550/arXiv.2302.13971)
:::

## Gao et al. "The Pile"?

::: columns
::: {.column width="50%"}
![](the_pile.png)
:::

::: {.column width="50%"}
![](pile-books3.png)
![](pile-books3-2.png)
:::
:::

::: footer
[Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling.” arXiv, December 31, 2020. https://doi.org/10.48550/arXiv.2101.00027.](https://doi.org/10.48550/arXiv.2101.00027)
:::

## Gao et al. "The Pile"?

::: columns
::: {.column width="50%"}
![](presser-2020.png)
:::

::: {.column width="50%"}
![](pile-books3.png)
![](pile-books3-2.png)
:::
:::

::: footer
[Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling.” arXiv, December 31, 2020. https://doi.org/10.48550/arXiv.2101.00027.](https://doi.org/10.48550/arXiv.2101.00027)
:::

## What could go wrong?{.center}

## What could go wrong?

:::{.r-stack}
![](cnn-silverman.png){.center width="500" height="533"}
:::

::: footer
[“Sarah Silverman Sues OpenAI and Meta Alleging Copyright Infringement | CNN Business.” Accessed August 27, 2023. https://www.cnn.com/2023/07/10/tech/sarah-silverman-openai-meta-lawsuit/index.html.](https://www.cnn.com/2023/07/10/tech/sarah-silverman-openai-meta-lawsuit/index.html)
:::

## What could go wrong?

::: columns
::: {.column width="50%"}
![](atlantic-books3.png)
:::

::: {.column width="50%"}
"Would I forbid the teaching (if that is the word) of my stories to computers? Not even if I could. I might as well be [King Canute, forbidding the tide to come in](https://en.wikipedia.org/wiki/King_Canute_and_the_tide). Or a Luddite trying to stop industrial progress by hammering a steam loom to pieces." -- Stephen King
:::
:::

::: footer
[Reisner, Alex. “Revealed: The Authors Whose Pirated Books Are Powering Generative AI.” The Atlantic, August 19, 2023. https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/.](https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/)

[King, Stephen. “Stephen King: My Books Were Used to Train AI.” The Atlantic, August 23, 2023. https://www.theatlantic.com/books/archive/2023/08/stephen-king-books-ai-writing/675088/.](https://www.theatlantic.com/books/archive/2023/08/stephen-king-books-ai-writing/675088/)
:::

## (GPT-3) Language Models are Few-Shot Learners (2020)

:::{.r-stack}
![](gpt-3-few-shot.png){.center width="512" height="590"}
:::

::: footer
[Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” arXiv, July 22, 2020. https://doi.org/10.48550/arXiv.2005.14165.](https://doi.org/10.48550/arXiv.2005.14165)
:::


## (GPT-3) In-Context Learning

:::{.r-stack}
![](gpt3-incontext.png){.center width="850" height="512"}
:::

::: footer
[Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” arXiv, July 22, 2020. https://doi.org/10.48550/arXiv.2005.14165.](https://doi.org/10.48550/arXiv.2005.14165)
:::

## (GPT-3) Instruct-GPT Reinforcement Learning from Human Feedback

:::{.r-stack}
![](instruct-gpt.png){.center width="497" height="500"}
:::

::: footer
[Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. “Training Language Models to Follow Instructions with Human Feedback.” arXiv, March 4, 2022. https://doi.org/10.48550/arXiv.2203.02155.](https://doi.org/10.48550/arXiv.2203.02155.)
:::

## (GPT-3) Instruct-GPT Reinforcement Learning from Human Feedback

:::{.r-stack}
![](instruct-gpt-method.png){.center width="751" height="550"}
:::

::: footer
[Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. “Training Language Models to Follow Instructions with Human Feedback.” arXiv, March 4, 2022. https://doi.org/10.48550/arXiv.2203.02155.](https://doi.org/10.48550/arXiv.2203.02155.)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](sparks-agi.png){.center width="744" height="550"}
:::

::: footer
[Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, et al. “Sparks of Artificial General Intelligence: Early Experiments with GPT-4.” arXiv, April 13, 2023. https://doi.org/10.48550/arXiv.2303.12712.](https://doi.org/10.48550/arXiv.2303.12712)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](sparks_unicorn.png){.center width="1065" height="550"}
:::

::: footer
[Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, et al. “Sparks of Artificial General Intelligence: Early Experiments with GPT-4.” arXiv, April 13, 2023. https://doi.org/10.48550/arXiv.2303.12712.](https://doi.org/10.48550/arXiv.2303.12712)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](sparks_unicorn_time.png){.center width="1000" height="338"}
:::

::: footer
[Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, et al. “Sparks of Artificial General Intelligence: Early Experiments with GPT-4.” arXiv, April 13, 2023. https://doi.org/10.48550/arXiv.2303.12712.](https://doi.org/10.48550/arXiv.2303.12712)
:::

## (GPT-4) "Sparks of AGI"?

:::{.r-stack}
![](youtube-sparks.png){.center width="723" height="550"}
:::

::: footer
[Sparks of AGI: Early Experiments with GPT-4, 2023. https://www.youtube.com/watch?v=qbIk7-JPB2c.](https://www.youtube.com/watch?v=qbIk7-JPB2c)
:::

## LLM Behavior Changes with Time!

:::{.r-stack}
![](../tai_testing/gpt_behavior_drift_chart.png){.center width="591" height="650"}
:::

::: footer
[Chen, Lingjiao, Matei Zaharia, and James Zou. “How Is ChatGPT’s Behavior Changing over Time?” arXiv, July 18, 2023. http://arxiv.org/abs/2307.09009.](http://arxiv.org/abs/2307.09009)
:::

## (GPT-4) OpenAI Reinforcement Learning -- "Towards TruthGPT"

:::{.r-stack}
![](youtube-truthgpt.png){.center width="812" height="600"}
:::

::: footer
[John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges, 2023. https://www.youtube.com/watch?v=hhiLw5Q_UFg.](https://www.youtube.com/watch?v=hhiLw5Q_UFg.)
:::

## (GPT-4) OpenAI Reinforcement Learning -- "Conceptual Models"

:::{.r-stack}
![](youtube-truthgpt-kg.png){.center width="828" height="600"}
:::

::: footer
[John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges, 2023. https://www.youtube.com/watch?v=hhiLw5Q_UFg.](https://www.youtube.com/watch?v=hhiLw5Q_UFg.)
:::


## (Claude) "Constitutional AI"

:::{.r-stack}
![](constitutional_ai.png){.center width="585" height="650"}
:::

::: footer
[Anthropic. “Claude’s Constitution.” Accessed August 28, 2023. https://www.anthropic.com/index/claudes-constitution.](https://www.anthropic.com/index/claudes-constitution)

[Bai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, et al. “Constitutional AI: Harmlessness from AI Feedback.” arXiv, December 15, 2022. https://doi.org/10.48550/arXiv.2212.08073.](https://doi.org/10.48550/arXiv.2212.08073)
:::

## (Claude) "Foundation Model in AWS Bedrock"

::: columns
::: {.column width="40%"}
![](claude-2.png)
:::

::: {.column width="60%"}
![](claude-2-bedrock.png){.center width="405" height="550"}
:::
:::


::: footer
[Anthropic. “Claude 2.” Accessed August 28, 2023. https://www.anthropic.com/index/claude-2.](https://www.anthropic.com/index/claude-2)

[Anthropic. “Claude 2 on Amazon Bedrock.” Accessed August 28, 2023. https://www.anthropic.com/index/claude-2-amazon-bedrock.
](https://www.anthropic.com/index/claude-2-amazon-bedrock)
:::

## (Llama 2)"Open-License" Large Language Models

::: {.r-stack}
![](llama2.png){.center width="617" height="650" }
:::

::: footer
[Meta AI. “Meta and Microsoft Introduce the Next Generation of Llama.” Accessed August 28, 2023. https://ai.meta.com/blog/llama-2/.
](https://ai.meta.com/blog/llama-2/)

[Meta AI. “Llama 2.” Accessed August 28, 2023. https://ai.meta.com/llama-project.
](https://ai.meta.com/llama-project)
:::

## "The state of GPT" Recommendations

:::{.r-stack}
![](karpathy-guidance.png){.center width="1076" height="600"}
:::

::: footer
[Andrej Karpathy, "State of GPT" | BRK216HFS, Microsoft Build, 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::


## Reasoning...{.center}

## (GPT-3) Large Language Models are Zero Shot Reasoners (Chain-of-Thought Reasoning)

:::{.r-stack}
![](CoT-Reasoning.png){.center width="520" height="500"}
:::

::: footer
[Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. “Large Language Models Are Zero-Shot Reasoners.” arXiv, January 29, 2023. https://doi.org/10.48550/arXiv.2205.11916.](https://doi.org/10.48550/arXiv.2205.11916)
:::

## (GPT-3) Large Language Models are Zero Shot Reasoners (Chain-of-Thought Reasoning)

:::{.r-stack}
![](CoT-Reasoning-example.png){.center width="694" height=500"}
:::

::: footer
[Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. “Large Language Models Are Zero-Shot Reasoners.” arXiv, January 29, 2023. https://doi.org/10.48550/arXiv.2205.11916.](https://doi.org/10.48550/arXiv.2205.11916)
:::

## ReAct: Synergizing Reasoning and Acting in Language Models

:::{.r-stack}
![](ReAcT.png){.center width="639" height=600"}
:::

::: footer
[“ReAct: Synergizing Reasoning and Acting in Language Models.” Accessed August 28, 2023. https://react-lm.github.io/.](https://react-lm.github.io/.])

[Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629.](http://arxiv.org/abs/2210.03629)
:::

## ReAct: Synergizing Reasoning and Acting in Language Models

:::{.r-stack}
![](react-diagram.png){.center width="1000" height=270"}
:::

::: footer
[“ReAct: Synergizing Reasoning and Acting in Language Models.” Accessed August 28, 2023. https://react-lm.github.io/.](https://react-lm.github.io/.])

[Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629.](http://arxiv.org/abs/2210.03629)
:::

## ReAct: Synergizing Reasoning and Acting in Language Models

:::{.r-stack}
![](react-prompt.png){.center width="611" height=600"}
:::

::: footer
[“ReAct: Synergizing Reasoning and Acting in Language Models.” Accessed August 28, 2023. https://react-lm.github.io/.](https://react-lm.github.io/.])

[Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629.](http://arxiv.org/abs/2210.03629)
:::

## Prompt Engineering

:::{.r-stack}
![](prompt-engineering.png){.center width="625" height=600"}
:::

::: footer
“Prompt Engineering Guide.” Accessed August 22, 2023. https://www.promptingguide.ai/.
:::

## Large Language Models are Semantic Reasoners

:::{.r-stack}
![](semantic-reasoners.png){.center width="701" height=600"}
:::

::: footer
[Tang, Xiaojuan, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. “Large Language Models Are In-Context Semantic Reasoners Rather than Symbolic Reasoners.” arXiv, June 8, 2023. http://arxiv.org/abs/2305.14825.]()
:::

## Large Language Models are Semantic Reasoners


::: columns
::: {.column width="40%"}
![](semantic-reasoners-exp.png)
:::

::: {.column width="60%"}
![](semantic-reasoners-types.png)
:::
:::

::: footer
[Tang, Xiaojuan, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. “Large Language Models Are In-Context Semantic Reasoners Rather than Symbolic Reasoners.” arXiv, June 8, 2023. http://arxiv.org/abs/2305.14825.]()
:::

## Our Research...{.center}

## Retrevial Augmented Generation

:::{.r-stack}
![](rag-claude.png){.center width="900" height=683"}
:::

## Knowledge Graphs and LLMs -- Must Read!

:::{.r-stack}
![](../tai-tools/PLM_vs_KG.png){.center width="927" height="600"}
:::

::: footer
[Pan, Shirui, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. “Unifying Large Language Models and Knowledge Graphs: A Roadmap.” arXiv, June 14, 2023. http://arxiv.org/abs/2306.08302.](http://arxiv.org/abs/2306.08302)
:::

## Knowledge Graphs and Frameworks

:::{.r-stack}
![](kg-llm-frameworks.png){.center width="616" height="650"}
:::

::: footer
[Pan, Shirui, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. “Unifying Large Language Models and Knowledge Graphs: A Roadmap.” arXiv, June 14, 2023. http://arxiv.org/abs/2306.08302.](http://arxiv.org/abs/2306.08302)
:::



## What About Trusted AI?{.center}

## How to talk about LLMs? Must Read!

:::{.r-stack}
![](talking_about_llms.png){.center width="665" height="600"}
:::

::: footer
[Shanahan, Murray. “Talking About Large Language Models.” arXiv, February 16, 2023. https://doi.org/10.48550/arXiv.2212.03551.](https://doi.org/10.48550/arXiv.2212.03551)
:::
