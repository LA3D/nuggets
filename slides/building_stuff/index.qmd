---
title: "Building \"Stuff\"?"
author: "Charles F. Vardeman II"
institute: "Center for Research Computing, University of Notre Dame"
date: "2023-09-05"
embed-resources: true
format: 
    revealjs:
        width: 1600
        height: 900
        logo: crc-logo-tagline-1-alt.png
        theme: [simple, crc.scss]
        slide-number: true
categories: [Agents,trustedAI,Development]
---

## Slides Link: [https://la3d.github.io/nuggets/slideindex.html](https://la3d.github.io/nuggets/slides/building_stuff/#/title-slide) {.center}


## Building "Stuff"...

![](../tai_testing/effective_testing_ml_systems.png){fig-align="center"}

::: footer
Jeremy Jordan. "Effective Testing for Machine Learning Systems.," 

August 19, 2020. <https://www.jeremyjordan.me/testing-ml/>.
:::

## Building "Stuff"...


::: r-stack
![](software1-2023-09-01-1139.png){fig-align="center" .fragment .fade-out}

![](software2-2023-09-01-1139.png){fig-align="center" .fragment .fade-in}
:::

##

::: {style="font-size: 2.5em; text-align: center"}
::: {.fragment .strike}
::: {.fragment .fade-out}
Building Stuff?
:::
:::

::: {.fragment .fade-in}
Building Agents based on Large Language Models!
:::
:::

## You are <u>"almost"</u> here ⬇️

::: columns
::: {.column width="50%"}
![](php.png)
:::

::: {.column width="50%"}
![](netscape.png)
:::
:::

## Building "Agents" Involves Pre-trained Foundation Models...

:::{.r-stack}
![](../nuggets_review/foundation_models.png){.center width="1000" height="340"}
:::

::: footer
Zhou, Ce, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, et al. 

“A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.”

arXiv, May 1, 2023.[https://doi.org/10.48550/arXiv.2302.09419.](https://doi.org/10.48550/arXiv.2302.09419)
:::

## How do we "program" a Large Language Model? {.center}

## Autoregressive Large Language Model

"An autoregressive large language model (AR-LLM) is a type of neural network model that can generate natural language text. It has a very large number of parameters (billions or trillions) that are trained on a huge amount of text data from various sources. The main goal of an AR-LLM is to predict the next word or token based on the previous words or tokens in the input text. For example, if the input text is "The sky is", the AR-LLM might predict "blue" as the next word. AR-LLMs can also generate text from scratch by sampling words from a probability distribution. For example, if the input text is empty, the AR-LLM might generate "Once upon a time, there was a princess who lived in a castle." as the output text."[^1]

[^1]: Bing Chat, Accessed: 09-05-23 <https://sl.bing.net/vDC45Llquq>

## AR-LLMs can <u>simulate</u> "Turing Machines"

::: {.r-stack}
![](llm-turing-machine.png){.center width="960" height="287"}
:::

::: {.r-stack}
**Abstract:**
We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and, potentially, simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, $U_{15,2}$. <u>**A key aspect of the finding is that it does not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts**</u>.
:::

::: footer
Schuurmans, Dale. "Memory Augmented Large Language Models Are Computationally Universal."

arXiv, January 9, 2023. <https://doi.org/10.48550/arXiv.2301.04589>.
:::


## What <u>Kind</u> of LLM Agents are we trying to build?

::: incremental
-   Conversational Agents
-   *vs*. Cognitive Autonomous Agents
-   *vs*. Agents tuned for a Data Processing Task
:::

## 

::: {style="font-size: 2.5em; text-align: center"}
We will focus on Conversational Agents...
:::

## The Best Advice we can Give...

::: r-stack
![](../nuggets_review/state_of_gpt.png){.center width="800" height="510"}
:::

::: footer
Andrej Karpathy, "State of GPT" \| BRK216HFS, Microsoft Build, 2023.

[https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

##

::: r-stack
![](../nuggets_review/state_of_gpt_pipeline.png){.center width="900" height="510"}
:::

::: footer
Andrej Karpathy, "State of GPT" \| BRK216HFS, Microsoft Build, 2023.

[https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

## Caveat: You are at the <u>Edge</u> of Research and Practice! {.center}

## Prompt Engineering

"Prompt engineering is the process of designing and refining the prompts or input stimuli for a language model to generate specific types of output. Prompt engineering involves selecting appropriate keywords, providing context, and shaping the input in a way that encourages the model to produce the desired response and is a vital technique to actively shape the behavior and output of foundation models."^["Prompt Engineering for Foundation Models - Amazon SageMaker." Accessed September 4, 2023. [http://tiny.cc/p3mavz](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-prompt-engineering.html).]


## (GPT-3) Instruct-GPT Reinforcement Learning from Human Feedback

::: r-stack
![](../nuggets_review/instruct-gpt-method.png){.center width="751" height="550"}
:::

::: footer
Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al.

"Training Language Models to Follow Instructions with Human Feedback."

arXiv, March 4, 2022. <https://doi.org/10.48550/arXiv.2203.02155>.
:::

## Instruction Tuning Facilitate Conversational Agents to "Converse" in a Set Style! {.center}

## Tools for "Prompt Engineering"

::: r-stack
![](prompt_flow.png){.center width="641" height="650"}
:::

::: footer
Henry Zeng, Lauryn Gayhardt, Jill Grant "What is Azure Machine Learning prompt flow

(preview) - Azure Machine Learning," Jul. 02, 2023.

[http://tiny.cc/kelavz](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2) (accessed Sep. 04, 2023).
:::

## Tools for "Prompt Engineering"

::: r-stack
![](prompt-flow-lifecycle.png){.center width="1000" height="490"}
:::

::: footer
Henry Zeng, Lauryn Gayhardt, Jill Grant "What is Azure Machine Learning prompt flow

(preview) - Azure Machine Learning," Jul. 02, 2023.

[http://tiny.cc/kelavz](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2) (accessed Sep. 04, 2023).
:::

## Trusted "Prompt Engineering" for Conversational Agents
"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or "rails" for short) are specific ways of controlling the output of a large language model, such as not talking about politics, responding in a particular way to specific user requests, following a predefined dialog path, using a particular language style, extracting structured data, and more."^[“NeMo Guardrails.” NVIDIA Corporation, Sep. 05, 2023. Accessed: Sep. 05, 2023. [Online]. Available: [https://github.com/NVIDIA/NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)]

## Trusted "Prompt Engineering" for Conversational Agents

- **Building Trustworthy, Safe, and Secure LLM Conversational Systems:** The core
value of using NeMo Guardrails is the ability to write rails to guide conversations. You
can choose to define the behavior of your LLM-powered application on specific topics and prevent it from engaging in discussions on unwanted topics.

- **Connect models, chains, services, and more via actions:** NeMo Guardrails provides the ability to connect an LLM to other services (a.k.a. tools) seamlessly and securely.

::: footer
“NeMo Guardrails.” NVIDIA Corporation, Sep. 05, 2023. Accessed: Sep. 05, 2023. [Online]. 

Available: [https://github.com/NVIDIA/NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)
:::

## (GPT-3) Large Language Models are Zero Shot Reasoners (Chain-of-Thought Reasoning)

::: r-stack
![](../nuggets_review/CoT-Reasoning.png){.center width="520" height="500"}
:::

::: footer
Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.

"Large Language Models Are Zero-Shot Reasoners." arXiv, January 29, 2023.

[https://doi.org/10.48550/arXiv.2205.11916.](https://doi.org/10.48550/arXiv.2205.11916)
:::

## LLMs as Reasoners using Prompts!

![](../agents_summer23/GOT_prompting.png)

::: footer
Besta, Maciej, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, et al

"Graph of Thoughts: Solving Elaborate Problems with Large Language Models." arXiv, August 21, 2023.

[http://arxiv.org/abs/2308.09687.](http://arxiv.org/abs/2308.09687)
:::

## Prompt Engineering Guide

::: r-stack
![](../tai_testing/prompt_engineering.png){fig-align="center" width="678" height="700"}
:::

::: footer
"Prompt Engineering Guide -- Nextra."

<https://www.promptingguide.ai/> (accessed Sep. 04, 2023).
:::


## We want Large Language Models to be **Factual**!

-   **Fine-Tuning**: augment the **behavior** of the model
-   **Retrieval:** introduce new **knowledge** to the model
-   **Retreval Aware Training (RAT)** Fine-tune the model to **use** or **ignore retrieved content**

## Fine-Tuning Foundation Models

"Foundation models are computationally expensive and trained on a large, unlabeled corpus. Fine-tuning a pre-trained foundation model is an affordable way to take advantage of their broad capabilities while customizing a model on your own small, corpus. Fine-tuning is a customization method that involved further training and does change the weights of your model..."

"...There are two main approaches that you can take for fine-tuning depending on your use case and chosen foundation model. If you're interested in fine-tuning your model on domain-specific data, see **Domain adaptation fine-tuning**. If you're interested in instruction-based fine-tuning using prompt and response examples, see **Instruction-based fine-tuning**."^["Fine-tune a foundation model - Amazon SageMaker", [http://tiny.cc/grmavz](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html) (accessed Sep. 05, 2023).]



## Retrieval Augmented Generation (RAG)
"Foundation models are usually trained offline, making the model agnostic to any data that is created after the model was trained. Additionally, foundation models are trained on very general domain corpora, making them less effective for domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data in context. For more information about RAG model architectures"^[“Retrieval Augmented Generation (RAG) - Amazon SageMaker.” Accessed September 4, 2023. [http://tiny.cc/f3mavz](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html).]

## Retrieval Augmented Generation (RAG)
::: r-stack
![](jumpstart-fm-rag.jpg){fig-align="center"}
:::

::: footer
“Retrieval Augmented Generation (RAG) - Amazon SageMaker.” Accessed September 4, 2023.

[http://tiny.cc/f3mavz](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html).
:::

## LlamaIndex to Build Hybrid KGs

::: r-stack
![](../agents_summer23/llamaindex_customretreiver.png){fig-align="center" width="466" height="700"}
:::

::: footer
"Custom Retriever Combining KG Index and VectorStore Index

-   LlamaIndex 🦙 0.8.5.Post2." Accessed August 22, 2023.

[http://tiny.cc/oflavz.](https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html)
:::

## Gorilla: Retrieval Aware Training for APIs

::: r-stack
![](gorilla-api.png){fig-align="center" width="1000" height="643"}
:::

::: footer
S. Patil, "Gorilla: Large Language Model Connected with Massive APIs \[Project Website\]." Sep. 04, 2023.

Accessed: Sep. 04, 2023. \[Online\]. Available: <https://github.com/ShishirPatil/gorilla>
:::

## Gorilla: Retrieval Aware Training for APIs

::: r-stack
![](gorilla-api-example.png){fig-align="center" width="1000" height="478"}
:::

::: footer
S. Patil, "Gorilla: Large Language Model Connected with Massive APIs \[Project Website\]." Sep. 04, 2023.

Accessed: Sep. 04, 2023. \[Online\]. Available: <https://github.com/ShishirPatil/gorilla>
:::

## "Big Models" vs "Small Models"

::: incremental
-   Models as a service (Bedrock, OpenAI API, Anthropic Claude)
    -   Generally more difficult to "Fine-Tune" (GPT-3.5 turbo)[^2]
    -   Models are generally more capable (Factuality, Instructions, Reasoning)\]
    -   "Coin-operated" pay per/token
-   "Open License" 7B-70B Parameter Models
    -   Mostly based on Meta AI LLama or LLama 2 models
    -   Require more effort to work consistently
    -   Models can run on reduced hardware requirements
    -   Can be fine-tuned for task specific workflows
:::

[^2]: "GPT-3.5 Turbo fine-tuning and API updates." <https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates> (accessed Sep. 04, 2023).

## Small Models with custom grammar (llama.cpp)

JSON-Grammar

```
root   ::= object
value  ::= object | array | string | number | ("true" | "false" | "null") ws

object ::=
  "{" ws (
            string ":" ws value
    ("," ws string ":" ws value)*
  )? "}" ws

array  ::=
  "[" ws (
            value
    ("," ws value)*
  )? "]" ws

string ::=
  "\"" (
    [^"\\] |
    "\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes
  )* "\"" ws

number ::= ("-"? ([0-9] | [1-9] [0-9]*)) ("." [0-9]+)? ([eE] [-+]? [0-9]+)? ws

# Optional space: by convention, applied in this grammar after literal chars when allowed
ws ::= ([ \t\n] ws)?
```

::: footer
"speculative : add grammar support by ggerganov · Pull Request #2991 · ggerganov/llama.cpp,"

GitHub. <https://github.com/ggerganov/llama.cpp/pull/2991> (accessed Sep. 04, 2023).
:::

##  {background-video="https://user-images.githubusercontent.com/1991296/265271972-e197f832-8c06-4fe4-8b68-b7415b2e4638.mp4" background-size="contain"}

## "The state of GPT" Recommendations

::: r-stack
![](../nuggets_review/karpathy-guidance.png){.center width="1076" height="600"}
:::

::: footer
Andrej Karpathy, "State of GPT" \| BRK216HFS, Microsoft Build, 2023.

[https://www.youtube.com/watch?v=bZQun8Y4L2A.](https://youtu.be/bZQun8Y4L2A?si=9w68buJRCPUqAlch)
:::

## Open Source Community

::: columns
::: {.column width="30%"}
![](a16z_header.png)
:::

::: {.column width="70%"}
![](a16z_grants.png){width="502" height="650"}
:::
:::

::: footer
Matt Bronstein and Rajko Radovanovic, "Supporting the Open Source AI Community,"

Andreessen Horowitz, Aug. 30, 2023.

[http://tiny.cc/uflavz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (accessed Sep. 03, 2023).
:::

## On to the First Steps to Building LLM Based Applications... {.center}